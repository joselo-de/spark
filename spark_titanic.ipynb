{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark, let the system find it easily with findspark\n",
    "import findspark\n",
    "findspark.init('/home/hadoop/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark-titanic').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+--------------------+-------+-----------+--------------------+-----+----------+-----+------+\n",
      "|row_names|pclass|survived|                name|    age|   embarked|           home_dest| room|    ticket| boat|   sex|\n",
      "+---------+------+--------+--------------------+-------+-----------+--------------------+-----+----------+-----+------+\n",
      "|        1|   1st|       1|Allen, Miss Elisa...|29.0000|Southampton|        St Louis, MO|  B-5|24160 L221|    2|female|\n",
      "|        2|   1st|       0|Allison, Miss Hel...| 2.0000|Southampton|Montreal, PQ / Ch...|  C26|      null| null|female|\n",
      "|        3|   1st|       0|Allison, Mr Hudso...|30.0000|Southampton|Montreal, PQ / Ch...|  C26|      null|(135)|  male|\n",
      "|        4|   1st|       0|Allison, Mrs Huds...|25.0000|Southampton|Montreal, PQ / Ch...|  C26|      null| null|female|\n",
      "|        5|   1st|       1|Allison, Master H...| 0.9167|Southampton|Montreal, PQ / Ch...|  C22|      null|   11|  male|\n",
      "|        6|   1st|       1|  Anderson, Mr Harry|47.0000|Southampton|        New York, NY| E-12|      null|    3|  male|\n",
      "|        7|   1st|       1|Andrews, Miss Kor...|63.0000|Southampton|          Hudson, NY|  D-7| 13502 L77|   10|female|\n",
      "|        8|   1st|       0|Andrews, Mr Thoma...|39.0000|Southampton|         Belfast, NI| A-36|      null| null|  male|\n",
      "|        9|   1st|       1|Appleton, Mrs Edw...|58.0000|Southampton| Bayside, Queens, NY|C-101|      null|    2|female|\n",
      "|       10|   1st|       0|Artagaveytia, Mr ...|71.0000|  Cherbourg| Montevideo, Uruguay| null|      null| (22)|  male|\n",
      "+---------+------+--------+--------------------+-------+-----------+--------------------+-----+----------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data source. format accepts 'json', 'csv', 'txt'\n",
    "df = spark.read.format('csv').option('header', 'True').option('inferSchema', 'True').load('s3a://bigd-hadoop/titanic.csv')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+--------------------+------------------+-----------+------+\n",
      "|summary|pclass|           survived|                name|               age|   embarked|  room|\n",
      "+-------+------+-------------------+--------------------+------------------+-----------+------+\n",
      "|  count|  1313|               1313|                1313|              1313|        821|    77|\n",
      "|   mean|  null|  0.341964965727342|                null| 31.19418104265403|       null|2131.0|\n",
      "| stddev|  null|0.47454867068071604|                null|14.747525275652208|       null|   NaN|\n",
      "|    min|   1st|                  0|\"Brown, Mrs James...|            0.1667|  Cherbourg|  2131|\n",
      "|    max|   3rd|                  1|del Carlo, Mrs Se...|                NA|Southampton|   F-?|\n",
      "+-------+------+-------------------+--------------------+------------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can take a quick look at the data with describe()\n",
    "desc_df = df.describe()\n",
    "desc_df.select('summary','pclass','survived','name','age','embarked','room').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe rows:  1313\n",
      "columns:  11\n"
     ]
    }
   ],
   "source": [
    "# how many rows and columns?\n",
    "print('dataframe rows: ', df.count())\n",
    "print('columns: ', len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_names: integer (nullable = true)\n",
      " |-- pclass: string (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      " |-- home_dest: string (nullable = true)\n",
      " |-- room: string (nullable = true)\n",
      " |-- ticket: string (nullable = true)\n",
      " |-- boat: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look at the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the schema, the first thing we notice is that 'age' and 'class' are strings. We expected those to be numbers, so we need to transform them first.\n",
    "\n",
    "#### We will be showing the resulting dataframe everytime we make a tranformation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|pclass|\n",
      "+------+\n",
      "|   2nd|\n",
      "|   1st|\n",
      "|   3rd|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find out the different classes\n",
    "df.select('pclass').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use UDF (User Defined Functions) to create class_index (string to number conversion)\n",
    "\n",
    "\n",
    "An example of UDF using the Lambda Function:\n",
    "\n",
    "age_udf = udf(lambda age: 'young' if age <= 30 else 'senior', StringType())\n",
    "\n",
    "df = df.withColumn('age_group', age_udf(df.age)).show()\n",
    "\n",
    "source: Singh, P. (2019). Machine Learning with PySpark With Natural Language Processing and Recommender Systems. Berkeley: Apress. doi: https://doi.org/10.1007/978-1-4842-4131-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+\n",
      "|                name|pclass|class_index|\n",
      "+--------------------+------+-----------+\n",
      "|del Carlo, Mrs Se...|   2nd|          2|\n",
      "|del Carlo, Mr Seb...|   2nd|          2|\n",
      "|de Villiers, Mada...|   1st|          1|\n",
      "|de Brito, Mr Jose...|   2nd|          2|\n",
      "|      Zimmerman, Leo|   3rd|          3|\n",
      "|       Zievens, Rene|   3rd|          3|\n",
      "|     Zenn, Mr Philip|   3rd|          3|\n",
      "|Zakarian, Mr Mapr...|   3rd|          3|\n",
      "|  Zakarian, Mr Artun|   3rd|          3|\n",
      "| Zabour, Miss Tamini|   3rd|          3|\n",
      "| Zabour, Miss Hileni|   3rd|          3|\n",
      "|Yrois, Miss Henri...|   2nd|          2|\n",
      "|  Youssef, Mr Gerios|   3rd|          3|\n",
      "|Young, Miss Marie...|   1st|          1|\n",
      "| Yasbeck, Mrs Antoni|   3rd|          3|\n",
      "|  Yasbeck, Mr Antoni|   3rd|          3|\n",
      "|   Yalsevac, Mr Ivan|   3rd|          3|\n",
      "|   Wright, Mr George|   1st|          1|\n",
      "| Wright, Miss Marion|   2nd|          2|\n",
      "|    Woolner, Mr Hugh|   1st|          1|\n",
      "+--------------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, when, col\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType#, ByteType\n",
    "\n",
    "class_udf = udf(lambda pclass: 1 if pclass == '1st' else (2 if pclass == '2nd' else 3), IntegerType())\n",
    "df = df.withColumn('class_index', class_udf(df['pclass']))\n",
    "df.select('name','pclass','class_index').orderBy('name', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|survived|\n",
      "+--------+\n",
      "|       1|\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# next column 'survived'. It looks good with only 0's and 1's\n",
    "df.select('survived').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                name|count|\n",
      "+--------------------+-----+\n",
      "|     Kelly, Mr James|    2|\n",
      "| Connolly, Miss Kate|    2|\n",
      "|Carlsson, Mr Fran...|    2|\n",
      "|    Coxon, Mr Daniel|    1|\n",
      "|       Lang, Mr Fang|    1|\n",
      "|Brocklebank, Mr W...|    1|\n",
      "|  Moss, Albert Johan|    1|\n",
      "|Harbeck, Mr Willi...|    1|\n",
      "|Levy, Mr Rene Jac...|    1|\n",
      "|Padro y Manent, M...|    1|\n",
      "|  Olsson, Miss Elida|    1|\n",
      "|Salkjelsvik, Miss...|    1|\n",
      "|Long, Mr Milton C...|    1|\n",
      "|Porter, Mr Walter...|    1|\n",
      "|    Denkoff, Mr Mito|    1|\n",
      "|    Foley, Mr Joseph|    1|\n",
      "|Jonsson, Nils Hil...|    1|\n",
      "|Moubarek, Mrs George|    1|\n",
      "|  Peduzzi, Mr Joseph|    1|\n",
      "|Coleridge, Mr Reg...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can we extract some info from 'name'?\n",
    "df.groupBy('name').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 3 duplicates, just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe rows:  1313\n",
      "dataframe rows after duplicates drop:  1310\n"
     ]
    }
   ],
   "source": [
    "# count rows before and after dropping duplicates\n",
    "print('dataframe rows: ', df.count())\n",
    "\n",
    "df = df.dropDuplicates(['name'])\n",
    "\n",
    "print('dataframe rows after duplicates drop: ', df.count())\n",
    "# it was good to eliminate some duplicate rows but at the end we will drop this column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was helpful since we eliminated some duplicates but at the end, the column 'name' will not be considered for the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|    age|count|\n",
      "+-------+-----+\n",
      "|     NA|  679|\n",
      "|30.0000|   28|\n",
      "|18.0000|   25|\n",
      "|36.0000|   23|\n",
      "|24.0000|   22|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby 'age' just to find out how many NULL values do we have on that column\n",
    "df.groupBy('age').count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with the NULL values, we will follow the standard practice of creating a new imputed column. Values will depend on whether the column was imputed=1 (meaning the original value was NULL, then replaced) or imputed=0 (value not replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_names: integer (nullable = true)\n",
      " |-- pclass: string (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      " |-- home_dest: string (nullable = true)\n",
      " |-- room: string (nullable = true)\n",
      " |-- ticket: string (nullable = true)\n",
      " |-- boat: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- class_index: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first, transform 'age' from StringType to DoubleType\n",
    "df = df.withColumn('age', df['age'].cast(DoubleType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+\n",
      "|                name| age|age_imputed|\n",
      "+--------------------+----+-----------+\n",
      "|Harbeck, Mr Willi...|44.0|          0|\n",
      "|       Lang, Mr Fang|null|          1|\n",
      "|Levy, Mr Rene Jac...|null|          1|\n",
      "|Padro y Manent, M...|null|          1|\n",
      "|Salkjelsvik, Miss...|null|          1|\n",
      "|Allison, Miss Hel...| 2.0|          0|\n",
      "|Candee, Mrs Edwar...|53.0|          0|\n",
      "|Carter, Mrs Willi...|36.0|          0|\n",
      "|Hakkarainen, Mr P...|null|          1|\n",
      "|Landegren, Miss A...|null|          1|\n",
      "|  Thomas, Mr Charles|null|          1|\n",
      "|Colley, Mr Edward...|null|          1|\n",
      "|Harknett, Miss Alice|null|          1|\n",
      "|Rosblom, Miss Sal...|null|          1|\n",
      "|Brocklebank, Mr W...|35.0|          0|\n",
      "|    Coxon, Mr Daniel|59.0|          0|\n",
      "|   Hart, Mr Benjamin|43.0|          0|\n",
      "|Widener, Mr Georg...|50.0|          0|\n",
      "|Doyle, Miss Eliza...|24.0|          0|\n",
      "|Futrelle, Mr Jacques|37.0|          0|\n",
      "+--------------------+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create age_imputed column before filling null values\n",
    "from pyspark.sql import functions as F\n",
    "df = df.select(col('*'), F.when(df['age'] > 0, 0).otherwise(1).alias('age_imputed'))\n",
    "df.select(['name','age','age_imputed']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(format_number(avg(age), 2)='31.21')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the mean of 'age' to fill in the null values\n",
    "from pyspark.sql.functions import mean, format_number\n",
    "\n",
    "mean_age = df.select(format_number(mean(df['age']), 2)).collect()\n",
    "#mean_age = df.select(mean(df['age'])).collect()\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_age as a float\n",
    "mean_age = float(mean_age[0][0])\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could fill the NULL values on 'age' with mean_age\n",
    "* df = df.na.fill(mean_age, 'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+------------------+\n",
      "|                name| age|age_imputed|           new_age|\n",
      "+--------------------+----+-----------+------------------+\n",
      "|Harbeck, Mr Willi...|44.0|          0|              44.0|\n",
      "|       Lang, Mr Fang|null|          1|31.205890015847856|\n",
      "|Levy, Mr Rene Jac...|null|          1|31.205890015847856|\n",
      "|Padro y Manent, M...|null|          1|31.205890015847856|\n",
      "|Salkjelsvik, Miss...|null|          1|31.205890015847856|\n",
      "|Allison, Miss Hel...| 2.0|          0|               2.0|\n",
      "|Candee, Mrs Edwar...|53.0|          0|              53.0|\n",
      "|Carter, Mrs Willi...|36.0|          0|              36.0|\n",
      "|Hakkarainen, Mr P...|null|          1|31.205890015847856|\n",
      "|Landegren, Miss A...|null|          1|31.205890015847856|\n",
      "|  Thomas, Mr Charles|null|          1|31.205890015847856|\n",
      "|Colley, Mr Edward...|null|          1|31.205890015847856|\n",
      "|Harknett, Miss Alice|null|          1|31.205890015847856|\n",
      "|Rosblom, Miss Sal...|null|          1|31.205890015847856|\n",
      "|Brocklebank, Mr W...|35.0|          0|              35.0|\n",
      "|    Coxon, Mr Daniel|59.0|          0|              59.0|\n",
      "|   Hart, Mr Benjamin|43.0|          0|              43.0|\n",
      "|Widener, Mr Georg...|50.0|          0|              50.0|\n",
      "|Doyle, Miss Eliza...|24.0|          0|              24.0|\n",
      "|Futrelle, Mr Jacques|37.0|          0|              37.0|\n",
      "+--------------------+----+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or better use Spark's imputer feature\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=['age'], outputCols=['new_age'])\n",
    "model = imputer.fit(df)\n",
    "\n",
    "df = model.transform(df)\n",
    "df.select(['name','age','age_imputed','new_age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   embarked|count|\n",
      "+-----------+-----+\n",
      "|Southampton|  572|\n",
      "|       null|  491|\n",
      "|  Cherbourg|  203|\n",
      "| Queenstown|   44|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look at 'embarked': 3 categories + null\n",
    "df.groupBy('embarked').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------+\n",
      "|                name|   embarked|embarked_index|\n",
      "+--------------------+-----------+--------------+\n",
      "|Harbeck, Mr Willi...|Southampton|           0.0|\n",
      "|       Lang, Mr Fang|       null|           3.0|\n",
      "|Levy, Mr Rene Jac...|  Cherbourg|           1.0|\n",
      "|Padro y Manent, M...|  Cherbourg|           1.0|\n",
      "|Salkjelsvik, Miss...|       null|           3.0|\n",
      "|Allison, Miss Hel...|Southampton|           0.0|\n",
      "|Candee, Mrs Edwar...|  Cherbourg|           1.0|\n",
      "|Carter, Mrs Willi...|Southampton|           0.0|\n",
      "|Hakkarainen, Mr P...|       null|           3.0|\n",
      "|Landegren, Miss A...|       null|           3.0|\n",
      "|  Thomas, Mr Charles|       null|           3.0|\n",
      "|Colley, Mr Edward...|Southampton|           0.0|\n",
      "|Harknett, Miss Alice|       null|           3.0|\n",
      "|Rosblom, Miss Sal...|       null|           3.0|\n",
      "|Brocklebank, Mr W...|Southampton|           0.0|\n",
      "|    Coxon, Mr Daniel|Southampton|           0.0|\n",
      "|   Hart, Mr Benjamin|Southampton|           0.0|\n",
      "|Widener, Mr Georg...|  Cherbourg|           1.0|\n",
      "|Doyle, Miss Eliza...| Queenstown|           2.0|\n",
      "|Futrelle, Mr Jacques|Southampton|           0.0|\n",
      "+--------------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries for indexer creation\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# we can either impute the NULL values with whatever arbitrary value we want or let spark do it.\n",
    "# here we use handleInvalid=\"keep\" to account for those NULL values. It will automatically create a 4th category with all the Nulls\n",
    "emb_indexer = StringIndexer(inputCol='embarked', outputCol='embarked_index', handleInvalid=\"keep\")\n",
    "df = emb_indexer.fit(df).transform(df)\n",
    "df.select(['name','embarked','embarked_index']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           home_dest|count|\n",
      "+--------------------+-----+\n",
      "|                null|  558|\n",
      "|        New York, NY|   64|\n",
      "|              London|   14|\n",
      "|        Montreal, PQ|   10|\n",
      "|Cornwall / Akron, OH|    9|\n",
      "|       Paris, France|    9|\n",
      "|        Winnipeg, MB|    8|\n",
      "|Wiltshire, Englan...|    8|\n",
      "|    Philadelphia, PA|    8|\n",
      "|             Belfast|    7|\n",
      "| Sweden Winnipeg, MN|    7|\n",
      "|        Brooklyn, NY|    7|\n",
      "|Sweden Worcester, MA|    5|\n",
      "|          Ottawa, ON|    5|\n",
      "|      Youngstown, OH|    5|\n",
      "|Somerset / Bernar...|    5|\n",
      "|Rotherfield, Suss...|    5|\n",
      "|Bulgaria Chicago, IL|    5|\n",
      "|Haverford, PA / C...|    5|\n",
      "|Ruotsinphytaa, Fi...|    4|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# although less nulls than other columns, the problem with home_dest is lack of uniformity\n",
    "df.groupBy('home_dest').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+----------+\n",
      "|                name|           home_dest|home_imputed|         home_filled|home_index|\n",
      "+--------------------+--------------------+------------+--------------------+----------+\n",
      "|Harbeck, Mr Willi...|Seattle, WA / Tol...|           0|Seattle, WA / Tol...|     303.0|\n",
      "|       Lang, Mr Fang|                null|           1|             Unknown|       0.0|\n",
      "|Levy, Mr Rene Jac...|        Montreal, PQ|           0|        Montreal, PQ|       3.0|\n",
      "|Padro y Manent, M...|Spain / Havana, Cuba|           0|Spain / Havana, Cuba|      82.0|\n",
      "|Salkjelsvik, Miss...|                null|           1|             Unknown|       0.0|\n",
      "|Allison, Miss Hel...|Montreal, PQ / Ch...|           0|Montreal, PQ / Ch...|      39.0|\n",
      "|Candee, Mrs Edwar...|      Washington, DC|           0|      Washington, DC|      22.0|\n",
      "|Carter, Mrs Willi...|       Bryn Mawr, PA|           0|       Bryn Mawr, PA|      32.0|\n",
      "|Hakkarainen, Mr P...|                null|           1|             Unknown|       0.0|\n",
      "|Landegren, Miss A...|                null|           1|             Unknown|       0.0|\n",
      "|  Thomas, Mr Charles|                null|           1|             Unknown|       0.0|\n",
      "|Colley, Mr Edward...|        Victoria, BC|           0|        Victoria, BC|     167.0|\n",
      "|Harknett, Miss Alice|                null|           1|             Unknown|       0.0|\n",
      "|Rosblom, Miss Sal...|                null|           1|             Unknown|       0.0|\n",
      "|Brocklebank, Mr W...|Broomfield, Chelm...|           0|Broomfield, Chelm...|     189.0|\n",
      "|    Coxon, Mr Daniel|         Merrill, WI|           0|         Merrill, WI|     319.0|\n",
      "|   Hart, Mr Benjamin|Ilford, Essex / W...|           0|Ilford, Essex / W...|      66.0|\n",
      "|Widener, Mr Georg...|     Elkins Park, PA|           0|     Elkins Park, PA|      57.0|\n",
      "|Doyle, Miss Eliza...|Ireland New York, NY|           0|Ireland New York, NY|      23.0|\n",
      "|Futrelle, Mr Jacques|        Scituate, MA|           0|        Scituate, MA|     108.0|\n",
      "+--------------------+--------------------+------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can create an imputed column for the nulls and an index for the rest of the values\n",
    "df = df.select(col('*'), F.when(df['home_dest'].isNull(), 1).otherwise(0).alias('home_imputed'))\n",
    "\n",
    "# replace null values with \"Unknown\"\n",
    "df = df.select(col('*'), F.when(df['home_dest'].isNull(), 'Unknown').otherwise(df['home_dest']).alias('home_filled'))\n",
    "\n",
    "home_indexer = StringIndexer(inputCol='home_filled', outputCol='home_index')\n",
    "df = home_indexer.fit(df).transform(df)\n",
    "df.select(['name','home_dest','home_imputed','home_filled','home_index']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    room|count|\n",
      "+--------+-----+\n",
      "|    null| 1233|\n",
      "|    F-33|    4|\n",
      "|     C26|    3|\n",
      "|   E-101|    3|\n",
      "|   C-101|    3|\n",
      "|    B-49|    2|\n",
      "| B-58/60|    2|\n",
      "|B-51/3/5|    2|\n",
      "|    C-85|    2|\n",
      "|     D-?|    2|\n",
      "|     B-5|    2|\n",
      "|   C-126|    2|\n",
      "|    C-87|    2|\n",
      "|    C-93|    2|\n",
      "|   C-125|    2|\n",
      "|    C-83|    2|\n",
      "|    B-18|    2|\n",
      "|     C22|    2|\n",
      "|    D-35|    2|\n",
      "|     C-7|    2|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in 'room' we see a lot of null values, but we can still get some categories if we take only the first letter\n",
    "df.groupBy('room').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------+----------+----------+\n",
      "|                name|  room|room_imputed|room_categ|room_index|\n",
      "+--------------------+------+------------+----------+----------+\n",
      "|    Swane, Mr George|   F-?|           0|         F|       4.0|\n",
      "|  Lemore, Mrs Amelia|  F-33|           0|         F|       4.0|\n",
      "|Nye, Mrs Elizabet...|  F-33|           0|         F|       4.0|\n",
      "|Cook, Mrs Selena ...|  F-33|           0|         F|       4.0|\n",
      "| Brown, Miss Mildred|  F-33|           0|         F|       4.0|\n",
      "|      Mack, Mrs Mary|   E77|           0|         E|       3.0|\n",
      "|     Buss, Miss Kate|   E-?|           0|         E|       3.0|\n",
      "|  Anderson, Mr Harry|  E-12|           0|         E|       3.0|\n",
      "| Keane, Miss Nora A.| E-101|           0|         E|       3.0|\n",
      "|  Webber, Miss Susan| E-101|           0|         E|       3.0|\n",
      "|Troutt, Miss Edwi...| E-101|           0|         E|       3.0|\n",
      "|Hogeboom, Mrs Joh...|   D-?|           0|         D|       2.0|\n",
      "|Longley, Miss Gre...|   D-?|           0|         D|       2.0|\n",
      "|Andrews, Miss Kor...|   D-7|           0|         D|       2.0|\n",
      "|Beesley, Mr Lawrence|  D-56|           0|         D|       2.0|\n",
      "|Nourney, Mr Alfre...|  D-38|           0|         D|       2.0|\n",
      "|Beckwith, Mr Rich...|  D-35|           0|         D|       2.0|\n",
      "|Beckwith, Mrs Ric...|  D-35|           0|         D|       2.0|\n",
      "|Borebank, Mr John...|D-21/2|           0|         D|       2.0|\n",
      "|Allison, Miss Hel...|   C26|           0|         C|       0.0|\n",
      "+--------------------+------+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create room_imputed column before filling nulls\n",
    "df = df.select(col('*'), F.when(df['room'].isNull(), 1).otherwise(0).alias('room_imputed'))\n",
    "\n",
    "# create room_category taking the first letter from room_number\n",
    "from pyspark.sql.functions import col, substring\n",
    "df = df.select(col('*'), substring(col('room'), 0, 1).alias('room_categ'))\n",
    "\n",
    "# create index for room\n",
    "# we use again handleInvalid=\"keep\" to include null values in the index\n",
    "room_indexer = StringIndexer(inputCol='room_categ', outputCol='room_index', handleInvalid=\"keep\")\n",
    "df = room_indexer.fit(df).transform(df)\n",
    "df.select('name','room','room_imputed','room_categ','room_index').orderBy('room', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|           ticket|count|\n",
      "+-----------------+-----+\n",
      "|             null| 1241|\n",
      "| 17608 L262 7s 6d|    5|\n",
      "|       230136 L39|    4|\n",
      "|       230080 L26|    3|\n",
      "|17754 L224 10s 6d|    3|\n",
      "|        13502 L77|    3|\n",
      "|    28220 L32 10s|    3|\n",
      "| 17582 L153 9s 3d|    3|\n",
      "|       24160 L221|    3|\n",
      "|     13529 L26 5s|    3|\n",
      "|    17755 L512 6s|    2|\n",
      "|  36973 L83 9s 6d|    2|\n",
      "|17483 L221 15s 7d|    2|\n",
      "|            17483|    2|\n",
      "|           392091|    2|\n",
      "|111361 L57 19s 7d|    2|\n",
      "|            17754|    2|\n",
      "| 17610 L27 15s 5d|    1|\n",
      "| 17593 L56 18s 7d|    1|\n",
      "| 17591 L50 9s 11d|    1|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 'ticket' is mostly NULL's. we'll just drop it\n",
    "df.groupBy('ticket').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|boat|count|\n",
      "+----+-----+\n",
      "|null|  963|\n",
      "|   4|   27|\n",
      "|   5|   27|\n",
      "|   7|   22|\n",
      "|   3|   19|\n",
      "|   8|   18|\n",
      "|   6|   17|\n",
      "|  11|   17|\n",
      "|  13|   16|\n",
      "|   9|   15|\n",
      "|  14|   15|\n",
      "|  12|   13|\n",
      "|   2|   11|\n",
      "|   D|   10|\n",
      "|  10|    8|\n",
      "|   B|    6|\n",
      "|   C|    6|\n",
      "|  15|    6|\n",
      "|   A|    5|\n",
      "| 5/7|    4|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extracting some data from 'boat' looks really challenging. we can try just to index it\n",
    "df.groupBy('boat').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                name| boat|boat_index|\n",
      "+--------------------+-----+----------+\n",
      "|\"Brown, Mrs James...|    6|       6.0|\n",
      "|  Abbing, Mr Anthony| null|      99.0|\n",
      "|Abbott, Master Eu...| null|      99.0|\n",
      "|Abbott, Mr Rossmo...|(190)|      62.0|\n",
      "|Abbott, Mrs Stant...|    A|      17.0|\n",
      "|Abelseth, Miss An...|   16|      21.0|\n",
      "|  Abelseth, Mr Olaus|    A|      17.0|\n",
      "|  Abelson, Mr Samuel| null|      99.0|\n",
      "|Abelson, Mrs Samu...|   12|      10.0|\n",
      "|Abraham, Mrs Jose...| null|      99.0|\n",
      "|Abrahamsson, Mr A...|   15|      14.0|\n",
      "|Adahl, Mr Mauritz...| (72)|      52.0|\n",
      "|      Adams, Mr John|(103)|      85.0|\n",
      "|Ahlin, Mrs Johann...| null|      99.0|\n",
      "|       Ahmed, Mr Ali| null|      99.0|\n",
      "| Aijo-Nirva, Mr Isak| null|      99.0|\n",
      "|  Aks, Master Philip|   11|       5.0|\n",
      "|Aks, Mrs Sam (Lea...|   13|       7.0|\n",
      "|Aldworth, Mr Char...| null|      99.0|\n",
      "|Alexander, Mr Wil...| null|      99.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create index for boat\n",
    "boat_indexer = StringIndexer(inputCol='boat', outputCol='boat_index', handleInvalid=\"keep\")\n",
    "df = boat_indexer.fit(df).transform(df)\n",
    "df.select('name','boat','boat_index').orderBy('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   sex|count|\n",
      "+------+-----+\n",
      "|  male|  848|\n",
      "|female|  462|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final column to transform: 'sex'\n",
    "df.groupBy('sex').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+\n",
      "|                name|   sex|sex_index|\n",
      "+--------------------+------+---------+\n",
      "|Harbeck, Mr Willi...|  male|      0.0|\n",
      "|       Lang, Mr Fang|  male|      0.0|\n",
      "|Levy, Mr Rene Jac...|  male|      0.0|\n",
      "|Padro y Manent, M...|  male|      0.0|\n",
      "|Salkjelsvik, Miss...|female|      1.0|\n",
      "|Allison, Miss Hel...|female|      1.0|\n",
      "|Candee, Mrs Edwar...|female|      1.0|\n",
      "|Carter, Mrs Willi...|female|      1.0|\n",
      "|Hakkarainen, Mr P...|  male|      0.0|\n",
      "|Landegren, Miss A...|female|      1.0|\n",
      "|  Thomas, Mr Charles|  male|      0.0|\n",
      "|Colley, Mr Edward...|  male|      0.0|\n",
      "|Harknett, Miss Alice|female|      1.0|\n",
      "|Rosblom, Miss Sal...|female|      1.0|\n",
      "|Brocklebank, Mr W...|  male|      0.0|\n",
      "|    Coxon, Mr Daniel|  male|      0.0|\n",
      "|   Hart, Mr Benjamin|  male|      0.0|\n",
      "|Widener, Mr Georg...|  male|      0.0|\n",
      "|Doyle, Miss Eliza...|female|      1.0|\n",
      "|Futrelle, Mr Jacques|  male|      0.0|\n",
      "+--------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create index for sex\n",
    "sex_indexer = StringIndexer(inputCol='sex', outputCol='sex_index')\n",
    "df = sex_indexer.fit(df).transform(df)\n",
    "df.select('name','sex','sex_index').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!   \n",
    "### For all index columns created, now transform them to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_names',\n",
       " 'pclass',\n",
       " 'survived',\n",
       " 'name',\n",
       " 'age',\n",
       " 'embarked',\n",
       " 'home_dest',\n",
       " 'room',\n",
       " 'ticket',\n",
       " 'boat',\n",
       " 'sex',\n",
       " 'class_index',\n",
       " 'age_imputed',\n",
       " 'new_age',\n",
       " 'embarked_index',\n",
       " 'home_imputed',\n",
       " 'home_filled',\n",
       " 'home_index',\n",
       " 'room_imputed',\n",
       " 'room_categ',\n",
       " 'room_index',\n",
       " 'boat_index',\n",
       " 'sex_index']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall all the columns on the dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all indexes to one_hot_encode vectors\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "class_encoder = OneHotEncoderEstimator(inputCols=['class_index'], outputCols=['class_vect'])\n",
    "df = class_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_encoder = OneHotEncoderEstimator(inputCols=['embarked_index'], outputCols=['embarked_vect'])\n",
    "df = emb_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_encoder = OneHotEncoderEstimator(inputCols=['home_index'], outputCols=['home_vect'])\n",
    "df = home_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_encoder = OneHotEncoderEstimator(inputCols=['room_index'], outputCols=['room_vect'])\n",
    "df = room_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat_encoder = OneHotEncoderEstimator(inputCols=['boat_index'], outputCols=['boat_vect'])\n",
    "df = boat_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_encoder = OneHotEncoderEstimator(inputCols=['sex_index'], outputCols=['sex_vect'])\n",
    "df = sex_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_names',\n",
       " 'pclass',\n",
       " 'survived',\n",
       " 'name',\n",
       " 'age',\n",
       " 'embarked',\n",
       " 'home_dest',\n",
       " 'room',\n",
       " 'ticket',\n",
       " 'boat',\n",
       " 'sex',\n",
       " 'class_index',\n",
       " 'age_imputed',\n",
       " 'new_age',\n",
       " 'embarked_index',\n",
       " 'home_imputed',\n",
       " 'home_filled',\n",
       " 'home_index',\n",
       " 'room_imputed',\n",
       " 'room_categ',\n",
       " 'room_index',\n",
       " 'boat_index',\n",
       " 'sex_index',\n",
       " 'class_vect',\n",
       " 'embarked_vect',\n",
       " 'home_vect',\n",
       " 'room_vect',\n",
       " 'boat_vect',\n",
       " 'sex_vect']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final step before creating any ML model: Single vector with all transformed columns and columns created when imputing values\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled features:\n",
      "['class_vect','age_imputed','new_age','embarked_vect','home_imputed','home_vect','room_imputed','room_vect','boat_vect','sex_vect']\n",
      "+--------------------+--------+\n",
      "|            features|survived|\n",
      "+--------------------+--------+\n",
      "|(488,[2,4,5,312,3...|       0|\n",
      "|(488,[3,4,8,9,380...|       1|\n",
      "|(488,[2,3,4,6,12,...|       0|\n",
      "|(488,[2,3,4,6,91,...|       1|\n",
      "|(488,[3,4,8,9,380...|       1|\n",
      "|(488,[1,4,5,48,38...|       0|\n",
      "|(488,[1,4,6,31,38...|       1|\n",
      "|(488,[1,4,5,41,38...|       1|\n",
      "|(488,[3,4,8,9,380...|       0|\n",
      "|(488,[3,4,8,9,380...|       1|\n",
      "|(488,[3,4,8,9,380...|       0|\n",
      "|(488,[1,3,4,5,176...|       0|\n",
      "|(488,[3,4,8,9,380...|       0|\n",
      "|(488,[3,4,8,9,380...|       0|\n",
      "|(488,[4,5,198,380...|       0|\n",
      "|(488,[4,5,328,380...|       0|\n",
      "|(488,[2,4,5,75,38...|       0|\n",
      "|(488,[1,4,6,66,38...|       0|\n",
      "|(488,[4,7,32,380]...|       0|\n",
      "|(488,[1,4,5,117,3...|       0|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first select only the labels and features we have created to this point\n",
    "df = df.select(['survived', 'class_index', 'age_imputed', 'new_age', 'embarked_index', 'home_imputed'\n",
    "               ,'home_index', 'room_imputed', 'room_index', 'boat_index', 'sex_index', 'class_vect'\n",
    "               ,'embarked_vect', 'home_vect', 'room_vect', 'boat_vect', 'sex_vect'])\n",
    "\n",
    "# create a single vector only with valid features (no indexes or labels)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['class_vect', 'age_imputed', 'new_age', 'embarked_vect', 'home_imputed', 'home_vect', 'room_imputed', 'room_vect', 'boat_vect', 'sex_vect'],\n",
    "    outputCol='features')\n",
    "\n",
    "final_df = assembler.transform(df)\n",
    "final_df = final_df.select('features', 'survived')\n",
    "\n",
    "print(\"Assembled features:\") \n",
    "print(\"['class_vect','age_imputed','new_age','embarked_vect','home_imputed','home_vect','room_imputed','room_vect','boat_vect','sex_vect']\")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally!!!\n",
    "Now we can create our regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data with an 70/30 ratio\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "train_data, test_data = final_df.randomSplit([0.7, 0.3], seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "lr_titanic = LogisticRegression(featuresCol='features', labelCol='survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "lr_model = lr_titanic.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can evaluate the performance of our model with a roc curve\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr_summary = lr_model.summary\n",
    "roc = lr_summary.roc.toPandas()\n",
    "plt.plot(roc['FPR'], roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve [training] ' + str(lr_summary.areaUnderROC));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get predictions with test_data\n",
    "lr_results = lr_model.transform(test_data)\n",
    "\n",
    "# the schema of results will give us the resulting vectors available for evaluation\n",
    "lr_results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       1|       1.0|\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look a the actual predictions\n",
    "lr_results.select('survived', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve [testing]: 0.8212245730605584\n"
     ]
    }
   ],
   "source": [
    "# last, we can evaluate our model with test_data\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='survived')\n",
    "\n",
    "lr_score = lr_eval.evaluate(lr_results)\n",
    "print('Area under the curve [testing]:', lr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's a pretty good model.\n",
    "\n",
    "Now, let's create other different models and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to evaluate performance of Decision Trees, Random Forest and Gradient Boosting\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 3 different models\n",
    "dtc = DecisionTreeClassifier(labelCol='survived', featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='survived', featuresCol='features')\n",
    "gbc = GBTClassifier(labelCol='survived', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbc_model = gbc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions with test_data\n",
    "dtc_preds = dtc_model.transform(test_data)\n",
    "rfc_preds = rfc_model.transform(test_data)\n",
    "gbc_preds = gbc_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can see the resulting vectors available for evaluation (actually those are same from linear_regression)\n",
    "dtc_preds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: 0.8212245730605584\n",
      "Decision Tree Classifier: 0.7026286646849683\n",
      "Random Forest Classifier: 0.8543014513026753\n",
      "Gradient Boosting Classifier: 0.926298303899283\n"
     ]
    }
   ],
   "source": [
    "# to evaluate our model we use again BinaryClassificationEvaluator\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol='survived')\n",
    "\n",
    "print('Logistic regression:', lr_score)\n",
    "print('Decision Tree Classifier:', binary_eval.evaluate(dtc_preds))\n",
    "print('Random Forest Classifier:', binary_eval.evaluate(rfc_preds))\n",
    "print('Gradient Boosting Classifier:', binary_eval.evaluate(gbc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest performs above than our logistic regression model\n",
    "\n",
    "### Gradient Boosting Classifier turns out to be the best one among the 4 chosen models\n",
    "\n",
    "### (Logistic Regression, Decision Trees, Random Forest, Gradient Boosting)\n",
    "\n",
    "As a final note, we should remember that we used the default parameters when creating the classification models.\n",
    "There are dozens of parameters we can start tuning in order to get a better score on each one of the models created"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
