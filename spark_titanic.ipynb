{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark, let the system find it easily with findspark\n",
    "import findspark\n",
    "findspark.init('/home/hadoop/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark-titanic').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+--------------------+-------+-----------+--------------------+-----+----------+-----+------+\n",
      "|row_names|pclass|survived|                name|    age|   embarked|           home_dest| room|    ticket| boat|   sex|\n",
      "+---------+------+--------+--------------------+-------+-----------+--------------------+-----+----------+-----+------+\n",
      "|        1|   1st|       1|Allen, Miss Elisa...|29.0000|Southampton|        St Louis, MO|  B-5|24160 L221|    2|female|\n",
      "|        2|   1st|       0|Allison, Miss Hel...| 2.0000|Southampton|Montreal, PQ / Ch...|  C26|      null| null|female|\n",
      "|        3|   1st|       0|Allison, Mr Hudso...|30.0000|Southampton|Montreal, PQ / Ch...|  C26|      null|(135)|  male|\n",
      "|        4|   1st|       0|Allison, Mrs Huds...|25.0000|Southampton|Montreal, PQ / Ch...|  C26|      null| null|female|\n",
      "|        5|   1st|       1|Allison, Master H...| 0.9167|Southampton|Montreal, PQ / Ch...|  C22|      null|   11|  male|\n",
      "|        6|   1st|       1|  Anderson, Mr Harry|47.0000|Southampton|        New York, NY| E-12|      null|    3|  male|\n",
      "|        7|   1st|       1|Andrews, Miss Kor...|63.0000|Southampton|          Hudson, NY|  D-7| 13502 L77|   10|female|\n",
      "|        8|   1st|       0|Andrews, Mr Thoma...|39.0000|Southampton|         Belfast, NI| A-36|      null| null|  male|\n",
      "|        9|   1st|       1|Appleton, Mrs Edw...|58.0000|Southampton| Bayside, Queens, NY|C-101|      null|    2|female|\n",
      "|       10|   1st|       0|Artagaveytia, Mr ...|71.0000|  Cherbourg| Montevideo, Uruguay| null|      null| (22)|  male|\n",
      "+---------+------+--------+--------------------+-------+-----------+--------------------+-----+----------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data source. format accepts 'json', 'csv', 'txt'\n",
    "df = spark.read.format('csv').option('header', 'True').option('inferSchema', 'True').load('s3a://bigd-hadoop/titanic.csv')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+--------------------+------------------+-----------+------+\n",
      "|summary|pclass|           survived|                name|               age|   embarked|  room|\n",
      "+-------+------+-------------------+--------------------+------------------+-----------+------+\n",
      "|  count|  1313|               1313|                1313|              1313|        821|    77|\n",
      "|   mean|  null|  0.341964965727342|                null| 31.19418104265403|       null|2131.0|\n",
      "| stddev|  null|0.47454867068071604|                null|14.747525275652208|       null|   NaN|\n",
      "|    min|   1st|                  0|\"Brown, Mrs James...|            0.1667|  Cherbourg|  2131|\n",
      "|    max|   3rd|                  1|del Carlo, Mrs Se...|                NA|Southampton|   F-?|\n",
      "+-------+------+-------------------+--------------------+------------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can take a quick look at the data with describe()\n",
    "desc_df = df.describe()\n",
    "desc_df.select('summary','pclass','survived','name','age','embarked','room').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe rows:  1313\n",
      "columns:  11\n"
     ]
    }
   ],
   "source": [
    "# how many rows and columns?\n",
    "print('dataframe rows: ', df.count())\n",
    "print('columns: ', len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_names: integer (nullable = true)\n",
      " |-- pclass: string (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      " |-- home_dest: string (nullable = true)\n",
      " |-- room: string (nullable = true)\n",
      " |-- ticket: string (nullable = true)\n",
      " |-- boat: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look at the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the schema, the first thing we notice is that 'age' and 'class' are strings. We expected those to be numbers, so we need to transform them first.\n",
    "\n",
    "#### We will be showing the resulting dataframe everytime we make a tranformation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|pclass|\n",
      "+------+\n",
      "|   2nd|\n",
      "|   1st|\n",
      "|   3rd|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find out the different classes\n",
    "df.select('pclass').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use UDF (User Defined Functions) to create class_index (string to number conversion)\n",
    "\n",
    "\n",
    "An example of UDF using the Lambda Function:\n",
    "\n",
    "age_udf = udf(lambda age: 'young' if age <= 30 else 'senior', StringType())\n",
    "\n",
    "df = df.withColumn('age_group', age_udf(df.age)).show()\n",
    "\n",
    "source: Singh, P. (2019). Machine Learning with PySpark With Natural Language Processing and Recommender Systems. Berkeley: Apress. doi: https://doi.org/10.1007/978-1-4842-4131-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+\n",
      "|                name|pclass|class_index|\n",
      "+--------------------+------+-----------+\n",
      "|del Carlo, Mrs Se...|   2nd|          2|\n",
      "|del Carlo, Mr Seb...|   2nd|          2|\n",
      "|de Villiers, Mada...|   1st|          1|\n",
      "|de Brito, Mr Jose...|   2nd|          2|\n",
      "|      Zimmerman, Leo|   3rd|          3|\n",
      "|       Zievens, Rene|   3rd|          3|\n",
      "|     Zenn, Mr Philip|   3rd|          3|\n",
      "|Zakarian, Mr Mapr...|   3rd|          3|\n",
      "|  Zakarian, Mr Artun|   3rd|          3|\n",
      "| Zabour, Miss Tamini|   3rd|          3|\n",
      "| Zabour, Miss Hileni|   3rd|          3|\n",
      "|Yrois, Miss Henri...|   2nd|          2|\n",
      "|  Youssef, Mr Gerios|   3rd|          3|\n",
      "|Young, Miss Marie...|   1st|          1|\n",
      "| Yasbeck, Mrs Antoni|   3rd|          3|\n",
      "|  Yasbeck, Mr Antoni|   3rd|          3|\n",
      "|   Yalsevac, Mr Ivan|   3rd|          3|\n",
      "|   Wright, Mr George|   1st|          1|\n",
      "| Wright, Miss Marion|   2nd|          2|\n",
      "|    Woolner, Mr Hugh|   1st|          1|\n",
      "+--------------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, when, col\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType#, ByteType\n",
    "\n",
    "class_udf = udf(lambda pclass: 1 if pclass == '1st' else (2 if pclass == '2nd' else 3), IntegerType())\n",
    "df = df.withColumn('class_index', class_udf(df['pclass']))\n",
    "df.select('name','pclass','class_index').orderBy('name', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|survived|\n",
      "+--------+\n",
      "|       1|\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# next column 'survived'. It looks good with only 0's and 1's\n",
    "df.select('survived').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                name|count|\n",
      "+--------------------+-----+\n",
      "|     Kelly, Mr James|    2|\n",
      "| Connolly, Miss Kate|    2|\n",
      "|Carlsson, Mr Fran...|    2|\n",
      "|Widener, Mr Georg...|    1|\n",
      "|Coleridge, Mr Reg...|    1|\n",
      "|Rosblom, Miss Sal...|    1|\n",
      "|Porter, Mr Walter...|    1|\n",
      "|   Hart, Mr Benjamin|    1|\n",
      "|Brocklebank, Mr W...|    1|\n",
      "|    Coxon, Mr Daniel|    1|\n",
      "|    Denkoff, Mr Mito|    1|\n",
      "|   Herman, Miss Kate|    1|\n",
      "|      Dibo, Mr Elias|    1|\n",
      "|    Kelly, Miss Mary|    1|\n",
      "| Nenkoff, Mr Christo|    1|\n",
      "|Odahl, Mr Nils Ma...|    1|\n",
      "| Thomas, Mr John, Jr|    1|\n",
      "|Long, Mr Milton C...|    1|\n",
      "|    Foley, Mr Joseph|    1|\n",
      "|Jonsson, Nils Hil...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can we extract some info from 'name'?\n",
    "df.groupBy('name').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 3 duplicates, just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe rows:  1313\n",
      "dataframe rows after duplicates drop:  1310\n"
     ]
    }
   ],
   "source": [
    "# count rows before and after dropping duplicates\n",
    "print('dataframe rows: ', df.count())\n",
    "\n",
    "df = df.dropDuplicates(['name'])\n",
    "\n",
    "print('dataframe rows after duplicates drop: ', df.count())\n",
    "# it was good to eliminate some duplicate rows but at the end we will drop this column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was helpful since we eliminated some duplicates but at the end, the column 'name' will not be considered for the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|    age|count|\n",
      "+-------+-----+\n",
      "|     NA|  679|\n",
      "|30.0000|   28|\n",
      "|18.0000|   25|\n",
      "|36.0000|   23|\n",
      "|24.0000|   22|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby 'age' just to find out how many NULL values do we have on that column\n",
    "df.groupBy('age').count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with the NULL values, we will follow the standard practice of creating a new imputed column. Values will depend on whether the column was imputed=1 (meaning the original value was NULL, then replaced) or imputed=0 (value not replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_names: integer (nullable = true)\n",
      " |-- pclass: string (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      " |-- home_dest: string (nullable = true)\n",
      " |-- room: string (nullable = true)\n",
      " |-- ticket: string (nullable = true)\n",
      " |-- boat: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- class_index: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first, transform 'age' from StringType to DoubleType\n",
    "df = df.withColumn('age', df['age'].cast(DoubleType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+\n",
      "|                name| age|age_imputed|\n",
      "+--------------------+----+-----------+\n",
      "|Harbeck, Mr Willi...|44.0|          0|\n",
      "|       Lang, Mr Fang|null|          1|\n",
      "|Levy, Mr Rene Jac...|null|          1|\n",
      "|Padro y Manent, M...|null|          1|\n",
      "|Salkjelsvik, Miss...|null|          1|\n",
      "|Allison, Miss Hel...| 2.0|          0|\n",
      "|Candee, Mrs Edwar...|53.0|          0|\n",
      "|Carter, Mrs Willi...|36.0|          0|\n",
      "|Hakkarainen, Mr P...|null|          1|\n",
      "|Landegren, Miss A...|null|          1|\n",
      "|  Thomas, Mr Charles|null|          1|\n",
      "|Colley, Mr Edward...|null|          1|\n",
      "|Harknett, Miss Alice|null|          1|\n",
      "|Rosblom, Miss Sal...|null|          1|\n",
      "|Brocklebank, Mr W...|35.0|          0|\n",
      "|    Coxon, Mr Daniel|59.0|          0|\n",
      "|   Hart, Mr Benjamin|43.0|          0|\n",
      "|Widener, Mr Georg...|50.0|          0|\n",
      "|Doyle, Miss Eliza...|24.0|          0|\n",
      "|Futrelle, Mr Jacques|37.0|          0|\n",
      "+--------------------+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create age_imputed column before filling null values\n",
    "from pyspark.sql import functions as F\n",
    "df = df.select(col('*'), F.when(df['age'] > 0, 0).otherwise(1).alias('age_imputed'))\n",
    "df.select(['name','age','age_imputed']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(format_number(avg(age), 2)='31.21')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the mean of 'age' to fill in the null values\n",
    "from pyspark.sql.functions import mean, format_number\n",
    "\n",
    "mean_age = df.select(format_number(mean(df['age']), 2)).collect()\n",
    "#mean_age = df.select(mean(df['age'])).collect()\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_age as a float\n",
    "mean_age = float(mean_age[0][0])\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could fill the NULL values on 'age' with mean_age\n",
    "* df = df.na.fill(mean_age, 'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+------------------+\n",
      "|                name| age|age_imputed|           new_age|\n",
      "+--------------------+----+-----------+------------------+\n",
      "|Harbeck, Mr Willi...|44.0|          0|              44.0|\n",
      "|       Lang, Mr Fang|null|          1|31.205890015847856|\n",
      "|Levy, Mr Rene Jac...|null|          1|31.205890015847856|\n",
      "|Padro y Manent, M...|null|          1|31.205890015847856|\n",
      "|Salkjelsvik, Miss...|null|          1|31.205890015847856|\n",
      "|Allison, Miss Hel...| 2.0|          0|               2.0|\n",
      "|Candee, Mrs Edwar...|53.0|          0|              53.0|\n",
      "|Carter, Mrs Willi...|36.0|          0|              36.0|\n",
      "|Hakkarainen, Mr P...|null|          1|31.205890015847856|\n",
      "|Landegren, Miss A...|null|          1|31.205890015847856|\n",
      "|  Thomas, Mr Charles|null|          1|31.205890015847856|\n",
      "|Colley, Mr Edward...|null|          1|31.205890015847856|\n",
      "|Harknett, Miss Alice|null|          1|31.205890015847856|\n",
      "|Rosblom, Miss Sal...|null|          1|31.205890015847856|\n",
      "|Brocklebank, Mr W...|35.0|          0|              35.0|\n",
      "|    Coxon, Mr Daniel|59.0|          0|              59.0|\n",
      "|   Hart, Mr Benjamin|43.0|          0|              43.0|\n",
      "|Widener, Mr Georg...|50.0|          0|              50.0|\n",
      "|Doyle, Miss Eliza...|24.0|          0|              24.0|\n",
      "|Futrelle, Mr Jacques|37.0|          0|              37.0|\n",
      "+--------------------+----+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or better use Spark's imputer feature\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=['age'], outputCols=['new_age'])\n",
    "model = imputer.fit(df)\n",
    "\n",
    "df = model.transform(df)\n",
    "df.select(['name','age','age_imputed','new_age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   embarked|count|\n",
      "+-----------+-----+\n",
      "|Southampton|  572|\n",
      "|       null|  491|\n",
      "|  Cherbourg|  203|\n",
      "| Queenstown|   44|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look at 'embarked': 3 categories + null\n",
    "df.groupBy('embarked').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------+\n",
      "|                name|   embarked|embarked_index|\n",
      "+--------------------+-----------+--------------+\n",
      "|Harbeck, Mr Willi...|Southampton|           0.0|\n",
      "|       Lang, Mr Fang|       null|           3.0|\n",
      "|Levy, Mr Rene Jac...|  Cherbourg|           1.0|\n",
      "|Padro y Manent, M...|  Cherbourg|           1.0|\n",
      "|Salkjelsvik, Miss...|       null|           3.0|\n",
      "|Allison, Miss Hel...|Southampton|           0.0|\n",
      "|Candee, Mrs Edwar...|  Cherbourg|           1.0|\n",
      "|Carter, Mrs Willi...|Southampton|           0.0|\n",
      "|Hakkarainen, Mr P...|       null|           3.0|\n",
      "|Landegren, Miss A...|       null|           3.0|\n",
      "|  Thomas, Mr Charles|       null|           3.0|\n",
      "|Colley, Mr Edward...|Southampton|           0.0|\n",
      "|Harknett, Miss Alice|       null|           3.0|\n",
      "|Rosblom, Miss Sal...|       null|           3.0|\n",
      "|Brocklebank, Mr W...|Southampton|           0.0|\n",
      "|    Coxon, Mr Daniel|Southampton|           0.0|\n",
      "|   Hart, Mr Benjamin|Southampton|           0.0|\n",
      "|Widener, Mr Georg...|  Cherbourg|           1.0|\n",
      "|Doyle, Miss Eliza...| Queenstown|           2.0|\n",
      "|Futrelle, Mr Jacques|Southampton|           0.0|\n",
      "+--------------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries for indexer creation\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# we can either impute the NULL values with whatever arbitrary value we want or let spark do it.\n",
    "# here we use handleInvalid=\"keep\" to account for those NULL values. It will automatically create a 4th category with all the Nulls\n",
    "emb_indexer = StringIndexer(inputCol='embarked', outputCol='embarked_index', handleInvalid=\"keep\")\n",
    "df = emb_indexer.fit(df).transform(df)\n",
    "df.select(['name','embarked','embarked_index']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           home_dest|count|\n",
      "+--------------------+-----+\n",
      "|                null|  558|\n",
      "|        New York, NY|   64|\n",
      "|              London|   14|\n",
      "|        Montreal, PQ|   10|\n",
      "|       Paris, France|    9|\n",
      "|Cornwall / Akron, OH|    9|\n",
      "|Wiltshire, Englan...|    8|\n",
      "|        Winnipeg, MB|    8|\n",
      "|    Philadelphia, PA|    8|\n",
      "| Sweden Winnipeg, MN|    7|\n",
      "|        Brooklyn, NY|    7|\n",
      "|             Belfast|    7|\n",
      "|Rotherfield, Suss...|    5|\n",
      "|          Ottawa, ON|    5|\n",
      "|Sweden Worcester, MA|    5|\n",
      "|Bulgaria Chicago, IL|    5|\n",
      "|Somerset / Bernar...|    5|\n",
      "|Haverford, PA / C...|    5|\n",
      "|      Youngstown, OH|    5|\n",
      "|  Syria New York, NY|    4|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# although less nulls than other columns, the problem with home_dest is lack of uniformity.\n",
    "# this column will receive the same treatment as the 'name' column. we'll just drop it\n",
    "df.groupBy('home_dest').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    room|count|\n",
      "+--------+-----+\n",
      "|    null| 1233|\n",
      "|    F-33|    4|\n",
      "|     C26|    3|\n",
      "|   C-101|    3|\n",
      "|   E-101|    3|\n",
      "|B-51/3/5|    2|\n",
      "|     B-5|    2|\n",
      "|    C-85|    2|\n",
      "|    C-93|    2|\n",
      "|    B-18|    2|\n",
      "| B-58/60|    2|\n",
      "|     D-?|    2|\n",
      "|     C22|    2|\n",
      "|   C-125|    2|\n",
      "|    D-35|    2|\n",
      "|   C-126|    2|\n",
      "|    C-83|    2|\n",
      "|    C-87|    2|\n",
      "|    B-49|    2|\n",
      "|     C-7|    2|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in 'room' we see a lot of null values, but we can still get some categories if we take only the first letter\n",
    "df.groupBy('room').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------+----------+----------+\n",
      "|                name|  room|room_imputed|room_categ|room_index|\n",
      "+--------------------+------+------------+----------+----------+\n",
      "|    Swane, Mr George|   F-?|           0|         F|       4.0|\n",
      "| Brown, Miss Mildred|  F-33|           0|         F|       4.0|\n",
      "|  Lemore, Mrs Amelia|  F-33|           0|         F|       4.0|\n",
      "|Nye, Mrs Elizabet...|  F-33|           0|         F|       4.0|\n",
      "|Cook, Mrs Selena ...|  F-33|           0|         F|       4.0|\n",
      "|      Mack, Mrs Mary|   E77|           0|         E|       3.0|\n",
      "|     Buss, Miss Kate|   E-?|           0|         E|       3.0|\n",
      "|  Anderson, Mr Harry|  E-12|           0|         E|       3.0|\n",
      "|Troutt, Miss Edwi...| E-101|           0|         E|       3.0|\n",
      "|  Webber, Miss Susan| E-101|           0|         E|       3.0|\n",
      "| Keane, Miss Nora A.| E-101|           0|         E|       3.0|\n",
      "|Longley, Miss Gre...|   D-?|           0|         D|       2.0|\n",
      "|Hogeboom, Mrs Joh...|   D-?|           0|         D|       2.0|\n",
      "|Andrews, Miss Kor...|   D-7|           0|         D|       2.0|\n",
      "|Beesley, Mr Lawrence|  D-56|           0|         D|       2.0|\n",
      "|Nourney, Mr Alfre...|  D-38|           0|         D|       2.0|\n",
      "|Beckwith, Mrs Ric...|  D-35|           0|         D|       2.0|\n",
      "|Beckwith, Mr Rich...|  D-35|           0|         D|       2.0|\n",
      "|Borebank, Mr John...|D-21/2|           0|         D|       2.0|\n",
      "|Allison, Miss Hel...|   C26|           0|         C|       0.0|\n",
      "+--------------------+------+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create room_imputed column before filling nulls\n",
    "df = df.select(col('*'), F.when(df['room'].isNull(), 1).otherwise(0).alias('room_imputed'))\n",
    "\n",
    "# create room_category taking the first letter from room_number\n",
    "from pyspark.sql.functions import col, substring\n",
    "df = df.select(col('*'), substring(col('room'), 0, 1).alias('room_categ'))\n",
    "\n",
    "# create index for room\n",
    "# we use again handleInvalid=\"keep\" to include null values in the index\n",
    "room_indexer = StringIndexer(inputCol='room_categ', outputCol='room_index', handleInvalid=\"keep\")\n",
    "df = room_indexer.fit(df).transform(df)\n",
    "df.select('name','room','room_imputed','room_categ','room_index').orderBy('room', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              ticket|count|\n",
      "+--------------------+-----+\n",
      "|                null| 1241|\n",
      "|    17608 L262 7s 6d|    5|\n",
      "|          230136 L39|    4|\n",
      "|          24160 L221|    3|\n",
      "|          230080 L26|    3|\n",
      "|       28220 L32 10s|    3|\n",
      "|           13502 L77|    3|\n",
      "|    17582 L153 9s 3d|    3|\n",
      "|   17754 L224 10s 6d|    3|\n",
      "|        13529 L26 5s|    3|\n",
      "|     36973 L83 9s 6d|    2|\n",
      "|   111361 L57 19s 7d|    2|\n",
      "|              392091|    2|\n",
      "|       17755 L512 6s|    2|\n",
      "|   17483 L221 15s 7d|    2|\n",
      "|               17483|    2|\n",
      "|               17754|    2|\n",
      "|112058 Complimentary|    1|\n",
      "|    17591 L50 9s 11d|    1|\n",
      "|              L15 1s|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 'ticket' is mostly NULL's. we'll drop it too\n",
    "df.groupBy('ticket').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|boat|count|\n",
      "+----+-----+\n",
      "|null|  963|\n",
      "|   4|   27|\n",
      "|   5|   27|\n",
      "|   7|   22|\n",
      "|   3|   19|\n",
      "|   8|   18|\n",
      "|  11|   17|\n",
      "|   6|   17|\n",
      "|  13|   16|\n",
      "|   9|   15|\n",
      "|  14|   15|\n",
      "|  12|   13|\n",
      "|   2|   11|\n",
      "|   D|   10|\n",
      "|  10|    8|\n",
      "|  15|    6|\n",
      "|   C|    6|\n",
      "|   B|    6|\n",
      "|   A|    5|\n",
      "|   1|    4|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extracting some data from 'boat' looks really challenging. we can try just to index it\n",
    "df.groupBy('boat').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                name| boat|boat_index|\n",
      "+--------------------+-----+----------+\n",
      "|\"Brown, Mrs James...|    6|       6.0|\n",
      "|  Abbing, Mr Anthony| null|      99.0|\n",
      "|Abbott, Master Eu...| null|      99.0|\n",
      "|Abbott, Mr Rossmo...|(190)|      62.0|\n",
      "|Abbott, Mrs Stant...|    A|      17.0|\n",
      "|Abelseth, Miss An...|   16|      21.0|\n",
      "|  Abelseth, Mr Olaus|    A|      17.0|\n",
      "|  Abelson, Mr Samuel| null|      99.0|\n",
      "|Abelson, Mrs Samu...|   12|      10.0|\n",
      "|Abraham, Mrs Jose...| null|      99.0|\n",
      "|Abrahamsson, Mr A...|   15|      14.0|\n",
      "|Adahl, Mr Mauritz...| (72)|      52.0|\n",
      "|      Adams, Mr John|(103)|      85.0|\n",
      "|Ahlin, Mrs Johann...| null|      99.0|\n",
      "|       Ahmed, Mr Ali| null|      99.0|\n",
      "| Aijo-Nirva, Mr Isak| null|      99.0|\n",
      "|  Aks, Master Philip|   11|       5.0|\n",
      "|Aks, Mrs Sam (Lea...|   13|       7.0|\n",
      "|Aldworth, Mr Char...| null|      99.0|\n",
      "|Alexander, Mr Wil...| null|      99.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create index for boat\n",
    "boat_indexer = StringIndexer(inputCol='boat', outputCol='boat_index', handleInvalid=\"keep\")\n",
    "df = boat_indexer.fit(df).transform(df)\n",
    "df.select('name','boat','boat_index').orderBy('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   sex|count|\n",
      "+------+-----+\n",
      "|  male|  848|\n",
      "|female|  462|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final column to transform: 'sex'\n",
    "df.groupBy('sex').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+\n",
      "|                name|   sex|sex_index|\n",
      "+--------------------+------+---------+\n",
      "|Harbeck, Mr Willi...|  male|      0.0|\n",
      "|       Lang, Mr Fang|  male|      0.0|\n",
      "|Levy, Mr Rene Jac...|  male|      0.0|\n",
      "|Padro y Manent, M...|  male|      0.0|\n",
      "|Salkjelsvik, Miss...|female|      1.0|\n",
      "|Allison, Miss Hel...|female|      1.0|\n",
      "|Candee, Mrs Edwar...|female|      1.0|\n",
      "|Carter, Mrs Willi...|female|      1.0|\n",
      "|Hakkarainen, Mr P...|  male|      0.0|\n",
      "|Landegren, Miss A...|female|      1.0|\n",
      "|  Thomas, Mr Charles|  male|      0.0|\n",
      "|Colley, Mr Edward...|  male|      0.0|\n",
      "|Harknett, Miss Alice|female|      1.0|\n",
      "|Rosblom, Miss Sal...|female|      1.0|\n",
      "|Brocklebank, Mr W...|  male|      0.0|\n",
      "|    Coxon, Mr Daniel|  male|      0.0|\n",
      "|   Hart, Mr Benjamin|  male|      0.0|\n",
      "|Widener, Mr Georg...|  male|      0.0|\n",
      "|Doyle, Miss Eliza...|female|      1.0|\n",
      "|Futrelle, Mr Jacques|  male|      0.0|\n",
      "+--------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create index for sex\n",
    "sex_indexer = StringIndexer(inputCol='sex', outputCol='sex_index')\n",
    "df = sex_indexer.fit(df).transform(df)\n",
    "df.select('name','sex','sex_index').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!   \n",
    "### For all index columns created, now transform them to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_names',\n",
       " 'pclass',\n",
       " 'survived',\n",
       " 'name',\n",
       " 'age',\n",
       " 'embarked',\n",
       " 'home_dest',\n",
       " 'room',\n",
       " 'ticket',\n",
       " 'boat',\n",
       " 'sex',\n",
       " 'class_index',\n",
       " 'age_imputed',\n",
       " 'new_age',\n",
       " 'embarked_index',\n",
       " 'room_imputed',\n",
       " 'room_categ',\n",
       " 'room_index',\n",
       " 'boat_index',\n",
       " 'sex_index']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall all the columns on the dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all indexes to one_hot_encode vectors\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "class_encoder = OneHotEncoderEstimator(inputCols=['class_index'], outputCols=['class_vect'])\n",
    "df = class_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_encoder = OneHotEncoderEstimator(inputCols=['embarked_index'], outputCols=['embarked_vect'])\n",
    "df = emb_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_encoder = OneHotEncoderEstimator(inputCols=['room_index'], outputCols=['room_vect'])\n",
    "df = room_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat_encoder = OneHotEncoderEstimator(inputCols=['boat_index'], outputCols=['boat_vect'])\n",
    "df = boat_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_encoder = OneHotEncoderEstimator(inputCols=['sex_index'], outputCols=['sex_vect'])\n",
    "df = sex_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_names',\n",
       " 'pclass',\n",
       " 'survived',\n",
       " 'name',\n",
       " 'age',\n",
       " 'embarked',\n",
       " 'home_dest',\n",
       " 'room',\n",
       " 'ticket',\n",
       " 'boat',\n",
       " 'sex',\n",
       " 'class_index',\n",
       " 'age_imputed',\n",
       " 'new_age',\n",
       " 'embarked_index',\n",
       " 'room_imputed',\n",
       " 'room_categ',\n",
       " 'room_index',\n",
       " 'boat_index',\n",
       " 'sex_index',\n",
       " 'class_vect',\n",
       " 'embarked_vect',\n",
       " 'room_vect',\n",
       " 'boat_vect',\n",
       " 'sex_vect']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final step before creating any ML model: Single vector with all transformed columns and columns created when imputing values\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled features:\n",
      "['class_vect', 'age_imputed', 'new_age', 'embarked_vect', 'room_imputed', 'room_vect', 'boat_vect', 'sex_vect']\n",
      "+--------------------+--------+\n",
      "|            features|survived|\n",
      "+--------------------+--------+\n",
      "|(116,[2,4,5,8,66,...|       0|\n",
      "|(116,[3,4,8,115],...|       1|\n",
      "|(116,[2,3,4,6,8,1...|       0|\n",
      "|(116,[2,3,4,6,8,1...|       1|\n",
      "|(116,[3,4,8],[1.0...|       1|\n",
      "|(116,[1,4,5,9],[1...|       0|\n",
      "|(116,[1,4,6,8,22]...|       1|\n",
      "|(116,[1,4,5,8,16]...|       1|\n",
      "|(116,[3,4,8,115],...|       0|\n",
      "|(116,[3,4,8],[1.0...|       1|\n",
      "|(116,[3,4,8,115],...|       0|\n",
      "|(116,[1,3,4,5,8,1...|       0|\n",
      "|(116,[3,4,8],[1.0...|       0|\n",
      "|(116,[3,4,8],[1.0...|       0|\n",
      "|(116,[4,5,8,115],...|       0|\n",
      "|(116,[4,5,8,115],...|       0|\n",
      "|(116,[2,4,5,8,115...|       0|\n",
      "|(116,[1,4,6,8,115...|       0|\n",
      "|(116,[4,7,8],[24....|       0|\n",
      "|(116,[1,4,5,8,115...|       0|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first select only the labels and features we have created to this point\n",
    "df = df.select(['survived', 'class_index', 'age_imputed', 'new_age', 'embarked_index'\n",
    "               ,'room_imputed', 'room_categ', 'room_index', 'boat_index', 'sex_index'\n",
    "               ,'class_vect', 'embarked_vect', 'room_vect', 'boat_vect', 'sex_vect'])\n",
    "\n",
    "# create a single vector only with valid features (no indexes or labels)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['class_vect', 'age_imputed', 'new_age', 'embarked_vect', 'room_imputed', 'room_vect', 'boat_vect', 'sex_vect'],\n",
    "    outputCol='features')\n",
    "\n",
    "final_df = assembler.transform(df)\n",
    "final_df = final_df.select('features', 'survived')\n",
    "\n",
    "print(\"Assembled features:\") \n",
    "print(\"['class_vect', 'age_imputed', 'new_age', 'embarked_vect', 'room_imputed', 'room_vect', 'boat_vect', 'sex_vect']\")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally!!!\n",
    "Now we can create our regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data with an 70/30 ratio\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "train_data, test_data = final_df.randomSplit([0.7, 0.3], seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "lr_titanic = LogisticRegression(featuresCol='features', labelCol='survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "lr_model = lr_titanic.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can evaluate the performance of our model with a roc curve\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr_summary = lr_model.summary\n",
    "roc = lr_summary.roc.toPandas()\n",
    "plt.plot(roc['FPR'], roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve [training] ' + str(lr_summary.areaUnderROC));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get predictions with test_data\n",
    "lr_results = lr_model.transform(test_data)\n",
    "\n",
    "# the schema of results will give us the resulting vectors available for evaluation\n",
    "lr_results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look a the actual predictions\n",
    "lr_results.select('survived', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve [testing]: 0.8635787533125936\n"
     ]
    }
   ],
   "source": [
    "# last, we can evaluate our model with test_data\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='survived')\n",
    "\n",
    "lr_score = lr_eval.evaluate(lr_results)\n",
    "print('Area under the curve [testing]:', lr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's a pretty good model.\n",
    "\n",
    "Now, let's create other different models and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to evaluate performance of Decision Trees, Random Forest and Gradient Boosting\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 3 different models\n",
    "dtc = DecisionTreeClassifier(labelCol='survived', featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='survived', featuresCol='features')\n",
    "gbc = GBTClassifier(labelCol='survived', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbc_model = gbc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions with test_data\n",
    "dtc_preds = dtc_model.transform(test_data)\n",
    "rfc_preds = rfc_model.transform(test_data)\n",
    "gbc_preds = gbc_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can see the resulting vectors available for evaluation (actually those are same from linear_regression)\n",
    "dtc_preds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: 0.8635787533125936\n",
      "Decision Tree Classifier: 0.44316741560087564\n",
      "Random Forest Classifier: 0.9011262818297039\n",
      "Gradient Boosting Classifier: 0.9347707109113953\n"
     ]
    }
   ],
   "source": [
    "# to evaluate our model we use again BinaryClassificationEvaluator\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol='survived')\n",
    "\n",
    "print('Logistic regression:', lr_score)\n",
    "print('Decision Tree Classifier:', binary_eval.evaluate(dtc_preds))\n",
    "print('Random Forest Classifier:', binary_eval.evaluate(rfc_preds))\n",
    "print('Gradient Boosting Classifier:', binary_eval.evaluate(gbc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### surprisingly, random forest performs just above than our logistic regression model\n",
    "\n",
    "### Gradient Boosting Classifier turns out to be the best one among the 4 chosen models\n",
    "\n",
    "### (Logistic Regression, Decision Trees, Random Forest, Gradient Boosting)\n",
    "\n",
    "As a final note, we should remember that we used the default parameters when creating the classification models.\n",
    "There are dozens of parameters we can start tuning in order to get a better score on each one of the models created"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
