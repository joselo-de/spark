{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark, let the system find it easily with findspark\n",
    "import findspark\n",
    "findspark.init('/home/ec2-user/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark-titanic').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+--------------------+-------+-----------+--------------------+-----+----------+-----+------+\n",
      "|row_names|pclass|survived|                name|    age|   embarked|           home_dest| room|    ticket| boat|   sex|\n",
      "+---------+------+--------+--------------------+-------+-----------+--------------------+-----+----------+-----+------+\n",
      "|        1|   1st|       1|Allen, Miss Elisa...|29.0000|Southampton|        St Louis, MO|  B-5|24160 L221|    2|female|\n",
      "|        2|   1st|       0|Allison, Miss Hel...| 2.0000|Southampton|Montreal, PQ / Ch...|  C26|      null| null|female|\n",
      "|        3|   1st|       0|Allison, Mr Hudso...|30.0000|Southampton|Montreal, PQ / Ch...|  C26|      null|(135)|  male|\n",
      "|        4|   1st|       0|Allison, Mrs Huds...|25.0000|Southampton|Montreal, PQ / Ch...|  C26|      null| null|female|\n",
      "|        5|   1st|       1|Allison, Master H...| 0.9167|Southampton|Montreal, PQ / Ch...|  C22|      null|   11|  male|\n",
      "|        6|   1st|       1|  Anderson, Mr Harry|47.0000|Southampton|        New York, NY| E-12|      null|    3|  male|\n",
      "|        7|   1st|       1|Andrews, Miss Kor...|63.0000|Southampton|          Hudson, NY|  D-7| 13502 L77|   10|female|\n",
      "|        8|   1st|       0|Andrews, Mr Thoma...|39.0000|Southampton|         Belfast, NI| A-36|      null| null|  male|\n",
      "|        9|   1st|       1|Appleton, Mrs Edw...|58.0000|Southampton| Bayside, Queens, NY|C-101|      null|    2|female|\n",
      "|       10|   1st|       0|Artagaveytia, Mr ...|71.0000|  Cherbourg| Montevideo, Uruguay| null|      null| (22)|  male|\n",
      "+---------+------+--------+--------------------+-------+-----------+--------------------+-----+----------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data source. format accepts 'json', 'csv', 'txt'\n",
    "df = spark.read.format('csv').option('header', 'True').option('inferSchema', 'True').load('s3a://bigd-hadoop/titanic.csv')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+--------------------+------------------+-----------+------+\n",
      "|summary|pclass|           survived|                name|               age|   embarked|  room|\n",
      "+-------+------+-------------------+--------------------+------------------+-----------+------+\n",
      "|  count|  1313|               1313|                1313|              1313|        821|    77|\n",
      "|   mean|  null|  0.341964965727342|                null| 31.19418104265403|       null|2131.0|\n",
      "| stddev|  null|0.47454867068071604|                null|14.747525275652208|       null|   NaN|\n",
      "|    min|   1st|                  0|\"Brown, Mrs James...|            0.1667|  Cherbourg|  2131|\n",
      "|    max|   3rd|                  1|del Carlo, Mrs Se...|                NA|Southampton|   F-?|\n",
      "+-------+------+-------------------+--------------------+------------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can take a quick look at the data with describe()\n",
    "desc_df = df.describe()\n",
    "desc_df.select('summary','pclass','survived','name','age','embarked','room').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe rows:  1313\n",
      "columns:  11\n"
     ]
    }
   ],
   "source": [
    "# how many rows and columns?\n",
    "print('dataframe rows: ', df.count())\n",
    "print('columns: ', len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_names: integer (nullable = true)\n",
      " |-- pclass: string (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      " |-- home_dest: string (nullable = true)\n",
      " |-- room: string (nullable = true)\n",
      " |-- ticket: string (nullable = true)\n",
      " |-- boat: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look at the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the schema, the first thing we notice is that 'age' and 'class' are strings. We expected those to be numbers, so we need to transform them first.\n",
    "\n",
    "#### We will be showing the resulting dataframe everytime we make a tranformation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|pclass|\n",
      "+------+\n",
      "|   2nd|\n",
      "|   1st|\n",
      "|   3rd|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find out the different classes\n",
    "df.select('pclass').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use UDF (User Defined Functions) to create class_index (string to number conversion)\n",
    "\n",
    "\n",
    "An example of UDF using the Lambda Function:\n",
    "\n",
    "age_udf = udf(lambda age: 'young' if age <= 30 else 'senior', StringType())\n",
    "\n",
    "df = df.withColumn('age_group', age_udf(df.age)).show()\n",
    "\n",
    "source: Singh, P. (2019). Machine Learning with PySpark With Natural Language Processing and Recommender Systems. Berkeley: Apress. doi: https://doi.org/10.1007/978-1-4842-4131-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+\n",
      "|                name|pclass|class_index|\n",
      "+--------------------+------+-----------+\n",
      "|del Carlo, Mrs Se...|   2nd|          2|\n",
      "|del Carlo, Mr Seb...|   2nd|          2|\n",
      "|de Villiers, Mada...|   1st|          1|\n",
      "|de Brito, Mr Jose...|   2nd|          2|\n",
      "|      Zimmerman, Leo|   3rd|          3|\n",
      "|       Zievens, Rene|   3rd|          3|\n",
      "|     Zenn, Mr Philip|   3rd|          3|\n",
      "|Zakarian, Mr Mapr...|   3rd|          3|\n",
      "|  Zakarian, Mr Artun|   3rd|          3|\n",
      "| Zabour, Miss Tamini|   3rd|          3|\n",
      "| Zabour, Miss Hileni|   3rd|          3|\n",
      "|Yrois, Miss Henri...|   2nd|          2|\n",
      "|  Youssef, Mr Gerios|   3rd|          3|\n",
      "|Young, Miss Marie...|   1st|          1|\n",
      "| Yasbeck, Mrs Antoni|   3rd|          3|\n",
      "|  Yasbeck, Mr Antoni|   3rd|          3|\n",
      "|   Yalsevac, Mr Ivan|   3rd|          3|\n",
      "|   Wright, Mr George|   1st|          1|\n",
      "| Wright, Miss Marion|   2nd|          2|\n",
      "|    Woolner, Mr Hugh|   1st|          1|\n",
      "+--------------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, when, col\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType#, ByteType\n",
    "\n",
    "class_udf = udf(lambda pclass: 1 if pclass == '1st' else (2 if pclass == '2nd' else 3), IntegerType())\n",
    "df = df.withColumn('class_index', class_udf(df['pclass']))\n",
    "df.select('name','pclass','class_index').orderBy('name', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|survived|\n",
      "+--------+\n",
      "|       1|\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# next column 'survived'. It looks good with only 0's and 1's\n",
    "df.select('survived').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                name|count|\n",
      "+--------------------+-----+\n",
      "|     Kelly, Mr James|    2|\n",
      "| Connolly, Miss Kate|    2|\n",
      "|Carlsson, Mr Fran...|    2|\n",
      "|Goldsmith, Mr Nathan|    1|\n",
      "|    Gill, Mr John W.|    1|\n",
      "| Ponesell, Mr Martin|    1|\n",
      "| Thomas, Mr John, Jr|    1|\n",
      "|Hagland, Mr Ingva...|    1|\n",
      "| Potchett, Mr George|    1|\n",
      "|Potter, Mrs Thoma...|    1|\n",
      "|Carter, Mr Willia...|    1|\n",
      "|Badman, Miss Emil...|    1|\n",
      "|Coleridge, Mr Reg...|    1|\n",
      "|   Herman, Miss Kate|    1|\n",
      "|      Dibo, Mr Elias|    1|\n",
      "|    Kelly, Miss Mary|    1|\n",
      "| Nenkoff, Mr Christo|    1|\n",
      "|Odahl, Mr Nils Ma...|    1|\n",
      "|Minahan, Miss Dai...|    1|\n",
      "| Icabad (Icabod), Ms|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can we extract some info from 'name'?\n",
    "df.groupBy('name').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 3 duplicates, just drop them. Before dropping them, let's see how the dropDuplicates function works. Here's one of the duplicated names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+-----------------------+-------+-----------+------------+----+------+----+----+-----------+\n",
      "|row_names|pclass|survived|name                   |age    |embarked   |home_dest   |room|ticket|boat|sex |class_index|\n",
      "+---------+------+--------+-----------------------+-------+-----------+------------+----+------+----+----+-----------+\n",
      "|45       |1st   |0       |Carlsson, Mr Frans Olof|33.0000|Southampton|New York, NY|null|null  |null|male|1          |\n",
      "|708      |3rd   |0       |Carlsson, Mr Frans Olof|33.0000|Southampton|New York, NY|null|null  |null|male|3          |\n",
      "+---------+------+--------+-----------------------+-------+-----------+------------+----+------+----+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\")).where(col(\"name\")=='Carlsson, Mr Frans Olof').show(truncate=False)\n",
    "#df.select(col(\"*\")).where(col(\"name\")=='Connolly, Miss Kate').show(truncate=False)\n",
    "#df.select(col(\"*\")).where(col(\"name\")=='Kelly, Mr James').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe rows:  1313\n",
      "dataframe rows after duplicates drop:  1310\n"
     ]
    }
   ],
   "source": [
    "# count rows before and after dropping duplicates\n",
    "print('dataframe rows: ', df.count())\n",
    "\n",
    "df = df.dropDuplicates(['name'])\n",
    "\n",
    "print('dataframe rows after duplicates drop: ', df.count())\n",
    "# it was good to eliminate some duplicate rows but at the end we will drop this column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying dropDuplicates, we see that only the first occurrence of 'Carlsson, Mr Frans Olof' was kept. The second occurrence was discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+--------------------+-------+-----------+------------+----+------+----+----+-----------+\n",
      "|row_names|pclass|survived|                name|    age|   embarked|   home_dest|room|ticket|boat| sex|class_index|\n",
      "+---------+------+--------+--------------------+-------+-----------+------------+----+------+----+----+-----------+\n",
      "|       45|   1st|       0|Carlsson, Mr Fran...|33.0000|Southampton|New York, NY|null|  null|null|male|          1|\n",
      "+---------+------+--------+--------------------+-------+-----------+------------+----+------+----+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\")).where(col(\"name\")=='Carlsson, Mr Frans Olof').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was helpful since we eliminated some duplicates but at the end, the column 'name' will not be considered for the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|    age|count|\n",
      "+-------+-----+\n",
      "|     NA|  679|\n",
      "|30.0000|   28|\n",
      "|18.0000|   25|\n",
      "|36.0000|   23|\n",
      "|24.0000|   22|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby 'age' just to find out how many NULL values do we have on that column\n",
    "df.groupBy('age').count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with the NULL values, we will follow the standard practice of creating a new imputed column. Values will depend on whether the column was imputed=1 (meaning the original value was NULL, then modified) or imputed=0 (value not modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_names: integer (nullable = true)\n",
      " |-- pclass: string (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      " |-- home_dest: string (nullable = true)\n",
      " |-- room: string (nullable = true)\n",
      " |-- ticket: string (nullable = true)\n",
      " |-- boat: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- class_index: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first, transform 'age' from StringType to DoubleType\n",
    "df = df.withColumn('age', df['age'].cast(DoubleType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+\n",
      "|                name| age|age_imputed|\n",
      "+--------------------+----+-----------+\n",
      "|Harbeck, Mr Willi...|44.0|          0|\n",
      "|       Lang, Mr Fang|null|          1|\n",
      "|Levy, Mr Rene Jac...|null|          1|\n",
      "|Padro y Manent, M...|null|          1|\n",
      "|Salkjelsvik, Miss...|null|          1|\n",
      "|Allison, Miss Hel...| 2.0|          0|\n",
      "|Candee, Mrs Edwar...|53.0|          0|\n",
      "|Carter, Mrs Willi...|36.0|          0|\n",
      "|Hakkarainen, Mr P...|null|          1|\n",
      "|Landegren, Miss A...|null|          1|\n",
      "|  Thomas, Mr Charles|null|          1|\n",
      "|Colley, Mr Edward...|null|          1|\n",
      "|Harknett, Miss Alice|null|          1|\n",
      "|Rosblom, Miss Sal...|null|          1|\n",
      "|Brocklebank, Mr W...|35.0|          0|\n",
      "|    Coxon, Mr Daniel|59.0|          0|\n",
      "|   Hart, Mr Benjamin|43.0|          0|\n",
      "|Widener, Mr Georg...|50.0|          0|\n",
      "|Doyle, Miss Eliza...|24.0|          0|\n",
      "|Futrelle, Mr Jacques|37.0|          0|\n",
      "+--------------------+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create age_imputed column before filling null values\n",
    "from pyspark.sql import functions as F\n",
    "df = df.select(col('*'), F.when(df['age'] > 0, 0).otherwise(1).alias('age_imputed'))\n",
    "df.select(['name','age','age_imputed']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(format_number(avg(age), 2)='31.21')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the mean of 'age' to fill in the null values\n",
    "from pyspark.sql.functions import mean, format_number\n",
    "\n",
    "mean_age = df.select(format_number(mean(df['age']), 2)).collect()\n",
    "#mean_age = df.select(mean(df['age'])).collect()\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.21"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_age as a float\n",
    "mean_age = float(mean_age[0][0])\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we could fill the NULL values on 'age' with mean_age\n",
    "* df = df.na.fill(mean_age, 'age').show()\n",
    "\n",
    "#### or better use Spark's imputer feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+------------------+\n",
      "|                name| age|age_imputed|           new_age|\n",
      "+--------------------+----+-----------+------------------+\n",
      "|Harbeck, Mr Willi...|44.0|          0|              44.0|\n",
      "|       Lang, Mr Fang|null|          1|31.205890015847856|\n",
      "|Levy, Mr Rene Jac...|null|          1|31.205890015847856|\n",
      "|Padro y Manent, M...|null|          1|31.205890015847856|\n",
      "|Salkjelsvik, Miss...|null|          1|31.205890015847856|\n",
      "|Allison, Miss Hel...| 2.0|          0|               2.0|\n",
      "|Candee, Mrs Edwar...|53.0|          0|              53.0|\n",
      "|Carter, Mrs Willi...|36.0|          0|              36.0|\n",
      "|Hakkarainen, Mr P...|null|          1|31.205890015847856|\n",
      "|Landegren, Miss A...|null|          1|31.205890015847856|\n",
      "|  Thomas, Mr Charles|null|          1|31.205890015847856|\n",
      "|Colley, Mr Edward...|null|          1|31.205890015847856|\n",
      "|Harknett, Miss Alice|null|          1|31.205890015847856|\n",
      "|Rosblom, Miss Sal...|null|          1|31.205890015847856|\n",
      "|Brocklebank, Mr W...|35.0|          0|              35.0|\n",
      "|    Coxon, Mr Daniel|59.0|          0|              59.0|\n",
      "|   Hart, Mr Benjamin|43.0|          0|              43.0|\n",
      "|Widener, Mr Georg...|50.0|          0|              50.0|\n",
      "|Doyle, Miss Eliza...|24.0|          0|              24.0|\n",
      "|Futrelle, Mr Jacques|37.0|          0|              37.0|\n",
      "+--------------------+----+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=['age'], outputCols=['new_age'])\n",
    "model = imputer.fit(df)\n",
    "\n",
    "df = model.transform(df)\n",
    "df.select(['name','age','age_imputed','new_age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   embarked|count|\n",
      "+-----------+-----+\n",
      "|Southampton|  572|\n",
      "|       null|  491|\n",
      "|  Cherbourg|  203|\n",
      "| Queenstown|   44|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look at 'embarked': 3 categories + null\n",
    "df.groupBy('embarked').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------+\n",
      "|                name|   embarked|embarked_index|\n",
      "+--------------------+-----------+--------------+\n",
      "|Harbeck, Mr Willi...|Southampton|           0.0|\n",
      "|       Lang, Mr Fang|       null|           3.0|\n",
      "|Levy, Mr Rene Jac...|  Cherbourg|           1.0|\n",
      "|Padro y Manent, M...|  Cherbourg|           1.0|\n",
      "|Salkjelsvik, Miss...|       null|           3.0|\n",
      "|Allison, Miss Hel...|Southampton|           0.0|\n",
      "|Candee, Mrs Edwar...|  Cherbourg|           1.0|\n",
      "|Carter, Mrs Willi...|Southampton|           0.0|\n",
      "|Hakkarainen, Mr P...|       null|           3.0|\n",
      "|Landegren, Miss A...|       null|           3.0|\n",
      "|  Thomas, Mr Charles|       null|           3.0|\n",
      "|Colley, Mr Edward...|Southampton|           0.0|\n",
      "|Harknett, Miss Alice|       null|           3.0|\n",
      "|Rosblom, Miss Sal...|       null|           3.0|\n",
      "|Brocklebank, Mr W...|Southampton|           0.0|\n",
      "|    Coxon, Mr Daniel|Southampton|           0.0|\n",
      "|   Hart, Mr Benjamin|Southampton|           0.0|\n",
      "|Widener, Mr Georg...|  Cherbourg|           1.0|\n",
      "|Doyle, Miss Eliza...| Queenstown|           2.0|\n",
      "|Futrelle, Mr Jacques|Southampton|           0.0|\n",
      "+--------------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries for indexer creation\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# we can either impute the NULL values with whatever arbitrary value we want or let spark do it.\n",
    "# here we use handleInvalid=\"keep\" to account for those NULL values. It will automatically create a 4th category with all the NULLS\n",
    "emb_indexer = StringIndexer(inputCol='embarked', outputCol='embarked_index', handleInvalid=\"keep\")\n",
    "df = emb_indexer.fit(df).transform(df)\n",
    "df.select(['name','embarked','embarked_index']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           home_dest|count|\n",
      "+--------------------+-----+\n",
      "|                null|  558|\n",
      "|        New York, NY|   64|\n",
      "|              London|   14|\n",
      "|        Montreal, PQ|   10|\n",
      "|       Paris, France|    9|\n",
      "|Cornwall / Akron, OH|    9|\n",
      "|        Winnipeg, MB|    8|\n",
      "|Wiltshire, Englan...|    8|\n",
      "|    Philadelphia, PA|    8|\n",
      "|             Belfast|    7|\n",
      "| Sweden Winnipeg, MN|    7|\n",
      "|        Brooklyn, NY|    7|\n",
      "|Sweden Worcester, MA|    5|\n",
      "|Haverford, PA / C...|    5|\n",
      "|      Youngstown, OH|    5|\n",
      "|Bulgaria Chicago, IL|    5|\n",
      "|Somerset / Bernar...|    5|\n",
      "|Rotherfield, Suss...|    5|\n",
      "|          Ottawa, ON|    5|\n",
      "|Guntur, India / B...|    4|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# although less nulls than other columns, the problem with home_dest is lack of uniformity.\n",
    "# this column will receive the same treatment as the 'name' column. we'll just drop it\n",
    "df.groupBy('home_dest').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    room|count|\n",
      "+--------+-----+\n",
      "|    null| 1233|\n",
      "|    F-33|    4|\n",
      "|     C26|    3|\n",
      "|   C-101|    3|\n",
      "|   E-101|    3|\n",
      "|    C-93|    2|\n",
      "|     C-7|    2|\n",
      "|    C-87|    2|\n",
      "|   C-125|    2|\n",
      "|    C-83|    2|\n",
      "| B-58/60|    2|\n",
      "|   C-126|    2|\n",
      "|    B-18|    2|\n",
      "|     C22|    2|\n",
      "|    C-85|    2|\n",
      "|    D-35|    2|\n",
      "|     B-5|    2|\n",
      "|    B-49|    2|\n",
      "|     D-?|    2|\n",
      "|B-51/3/5|    2|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in 'room' we see a lot of null values, but we can still get some categories if we take only the first letter\n",
    "df.groupBy('room').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------+----------+----------+\n",
      "|                name|  room|room_imputed|room_categ|room_index|\n",
      "+--------------------+------+------------+----------+----------+\n",
      "|    Swane, Mr George|   F-?|           0|         F|       4.0|\n",
      "|Cook, Mrs Selena ...|  F-33|           0|         F|       4.0|\n",
      "|Nye, Mrs Elizabet...|  F-33|           0|         F|       4.0|\n",
      "|  Lemore, Mrs Amelia|  F-33|           0|         F|       4.0|\n",
      "| Brown, Miss Mildred|  F-33|           0|         F|       4.0|\n",
      "|      Mack, Mrs Mary|   E77|           0|         E|       3.0|\n",
      "|     Buss, Miss Kate|   E-?|           0|         E|       3.0|\n",
      "|  Anderson, Mr Harry|  E-12|           0|         E|       3.0|\n",
      "| Keane, Miss Nora A.| E-101|           0|         E|       3.0|\n",
      "|Troutt, Miss Edwi...| E-101|           0|         E|       3.0|\n",
      "|  Webber, Miss Susan| E-101|           0|         E|       3.0|\n",
      "|Longley, Miss Gre...|   D-?|           0|         D|       2.0|\n",
      "|Hogeboom, Mrs Joh...|   D-?|           0|         D|       2.0|\n",
      "|Andrews, Miss Kor...|   D-7|           0|         D|       2.0|\n",
      "|Beesley, Mr Lawrence|  D-56|           0|         D|       2.0|\n",
      "|Nourney, Mr Alfre...|  D-38|           0|         D|       2.0|\n",
      "|Beckwith, Mr Rich...|  D-35|           0|         D|       2.0|\n",
      "|Beckwith, Mrs Ric...|  D-35|           0|         D|       2.0|\n",
      "|Borebank, Mr John...|D-21/2|           0|         D|       2.0|\n",
      "|Allison, Mrs Huds...|   C26|           0|         C|       0.0|\n",
      "+--------------------+------+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create room_imputed column before filling nulls\n",
    "df = df.select(col('*'), F.when(df['room'].isNull(), 1).otherwise(0).alias('room_imputed'))\n",
    "\n",
    "# create room_category taking the first letter from room_number\n",
    "from pyspark.sql.functions import col, substring\n",
    "df = df.select(col('*'), substring(col('room'), 0, 1).alias('room_categ'))\n",
    "\n",
    "# create index for room\n",
    "# we use again handleInvalid=\"keep\" to include null values in the index\n",
    "room_indexer = StringIndexer(inputCol='room_categ', outputCol='room_index', handleInvalid=\"keep\")\n",
    "df = room_indexer.fit(df).transform(df)\n",
    "df.select('name','room','room_imputed','room_categ','room_index').orderBy('room', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|           ticket|count|\n",
      "+-----------------+-----+\n",
      "|             null| 1241|\n",
      "| 17608 L262 7s 6d|    5|\n",
      "|       230136 L39|    4|\n",
      "|17754 L224 10s 6d|    3|\n",
      "| 17582 L153 9s 3d|    3|\n",
      "|       24160 L221|    3|\n",
      "|     13529 L26 5s|    3|\n",
      "|        13502 L77|    3|\n",
      "|       230080 L26|    3|\n",
      "|    28220 L32 10s|    3|\n",
      "|111361 L57 19s 7d|    2|\n",
      "|            17754|    2|\n",
      "|17483 L221 15s 7d|    2|\n",
      "|  36973 L83 9s 6d|    2|\n",
      "|            17483|    2|\n",
      "|    17755 L512 6s|    2|\n",
      "|           392091|    2|\n",
      "|    17604 L39 12s|    1|\n",
      "|           L15 1s|    1|\n",
      "|       229236 L13|    1|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 'ticket' is mostly NULL's. we'll drop it too\n",
    "df.groupBy('ticket').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|boat|count|\n",
      "+----+-----+\n",
      "|null|  963|\n",
      "|   4|   27|\n",
      "|   5|   27|\n",
      "|   7|   22|\n",
      "|   3|   19|\n",
      "|   8|   18|\n",
      "|   6|   17|\n",
      "|  11|   17|\n",
      "|  13|   16|\n",
      "|  14|   15|\n",
      "|   9|   15|\n",
      "|  12|   13|\n",
      "|   2|   11|\n",
      "|   D|   10|\n",
      "|  10|    8|\n",
      "|   B|    6|\n",
      "|   C|    6|\n",
      "|  15|    6|\n",
      "|   A|    5|\n",
      "| 5/7|    4|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extracting some data from 'boat' looks really challenging. we can try just to index it\n",
    "df.groupBy('boat').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                name| boat|boat_index|\n",
      "+--------------------+-----+----------+\n",
      "|\"Brown, Mrs James...|    6|       6.0|\n",
      "|  Abbing, Mr Anthony| null|      99.0|\n",
      "|Abbott, Master Eu...| null|      99.0|\n",
      "|Abbott, Mr Rossmo...|(190)|      62.0|\n",
      "|Abbott, Mrs Stant...|    A|      17.0|\n",
      "|Abelseth, Miss An...|   16|      21.0|\n",
      "|  Abelseth, Mr Olaus|    A|      17.0|\n",
      "|  Abelson, Mr Samuel| null|      99.0|\n",
      "|Abelson, Mrs Samu...|   12|      10.0|\n",
      "|Abraham, Mrs Jose...| null|      99.0|\n",
      "|Abrahamsson, Mr A...|   15|      14.0|\n",
      "|Adahl, Mr Mauritz...| (72)|      52.0|\n",
      "|      Adams, Mr John|(103)|      85.0|\n",
      "|Ahlin, Mrs Johann...| null|      99.0|\n",
      "|       Ahmed, Mr Ali| null|      99.0|\n",
      "| Aijo-Nirva, Mr Isak| null|      99.0|\n",
      "|  Aks, Master Philip|   11|       5.0|\n",
      "|Aks, Mrs Sam (Lea...|   13|       7.0|\n",
      "|Aldworth, Mr Char...| null|      99.0|\n",
      "|Alexander, Mr Wil...| null|      99.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create index for boat\n",
    "boat_indexer = StringIndexer(inputCol='boat', outputCol='boat_index', handleInvalid=\"keep\")\n",
    "df = boat_indexer.fit(df).transform(df)\n",
    "df.select('name','boat','boat_index').orderBy('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   sex|count|\n",
      "+------+-----+\n",
      "|  male|  848|\n",
      "|female|  462|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final column to transform: 'sex'\n",
    "df.groupBy('sex').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+\n",
      "|                name|   sex|sex_index|\n",
      "+--------------------+------+---------+\n",
      "|Harbeck, Mr Willi...|  male|      0.0|\n",
      "|       Lang, Mr Fang|  male|      0.0|\n",
      "|Levy, Mr Rene Jac...|  male|      0.0|\n",
      "|Padro y Manent, M...|  male|      0.0|\n",
      "|Salkjelsvik, Miss...|female|      1.0|\n",
      "|Allison, Miss Hel...|female|      1.0|\n",
      "|Candee, Mrs Edwar...|female|      1.0|\n",
      "|Carter, Mrs Willi...|female|      1.0|\n",
      "|Hakkarainen, Mr P...|  male|      0.0|\n",
      "|Landegren, Miss A...|female|      1.0|\n",
      "|  Thomas, Mr Charles|  male|      0.0|\n",
      "|Colley, Mr Edward...|  male|      0.0|\n",
      "|Harknett, Miss Alice|female|      1.0|\n",
      "|Rosblom, Miss Sal...|female|      1.0|\n",
      "|Brocklebank, Mr W...|  male|      0.0|\n",
      "|    Coxon, Mr Daniel|  male|      0.0|\n",
      "|   Hart, Mr Benjamin|  male|      0.0|\n",
      "|Widener, Mr Georg...|  male|      0.0|\n",
      "|Doyle, Miss Eliza...|female|      1.0|\n",
      "|Futrelle, Mr Jacques|  male|      0.0|\n",
      "+--------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create index for sex\n",
    "sex_indexer = StringIndexer(inputCol='sex', outputCol='sex_index')\n",
    "df = sex_indexer.fit(df).transform(df)\n",
    "df.select('name','sex','sex_index').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!   \n",
    "### For all index columns created, now transform them to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_names',\n",
       " 'pclass',\n",
       " 'survived',\n",
       " 'name',\n",
       " 'age',\n",
       " 'embarked',\n",
       " 'home_dest',\n",
       " 'room',\n",
       " 'ticket',\n",
       " 'boat',\n",
       " 'sex',\n",
       " 'class_index',\n",
       " 'age_imputed',\n",
       " 'new_age',\n",
       " 'embarked_index',\n",
       " 'room_imputed',\n",
       " 'room_categ',\n",
       " 'room_index',\n",
       " 'boat_index',\n",
       " 'sex_index']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall all the columns on the dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all indexes to one_hot_encode vectors\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "class_encoder = OneHotEncoderEstimator(inputCols=['class_index'], outputCols=['class_vect'])\n",
    "df = class_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_encoder = OneHotEncoderEstimator(inputCols=['embarked_index'], outputCols=['embarked_vect'])\n",
    "df = emb_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_encoder = OneHotEncoderEstimator(inputCols=['room_index'], outputCols=['room_vect'])\n",
    "df = room_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat_encoder = OneHotEncoderEstimator(inputCols=['boat_index'], outputCols=['boat_vect'])\n",
    "df = boat_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_encoder = OneHotEncoderEstimator(inputCols=['sex_index'], outputCols=['sex_vect'])\n",
    "df = sex_encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_names',\n",
       " 'pclass',\n",
       " 'survived',\n",
       " 'name',\n",
       " 'age',\n",
       " 'embarked',\n",
       " 'home_dest',\n",
       " 'room',\n",
       " 'ticket',\n",
       " 'boat',\n",
       " 'sex',\n",
       " 'class_index',\n",
       " 'age_imputed',\n",
       " 'new_age',\n",
       " 'embarked_index',\n",
       " 'room_imputed',\n",
       " 'room_categ',\n",
       " 'room_index',\n",
       " 'boat_index',\n",
       " 'sex_index',\n",
       " 'class_vect',\n",
       " 'embarked_vect',\n",
       " 'room_vect',\n",
       " 'boat_vect',\n",
       " 'sex_vect']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final step before creating any ML model: Single vector with all transformed columns and columns created when imputing values\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled features:\n",
      "['class_vect', 'age_imputed', 'new_age', 'embarked_vect', 'room_imputed', 'room_vect', 'boat_vect', 'sex_vect']\n",
      "+--------------------+--------+\n",
      "|            features|survived|\n",
      "+--------------------+--------+\n",
      "|(116,[2,4,5,8,66,...|       0|\n",
      "|(116,[3,4,8,115],...|       1|\n",
      "|(116,[2,3,4,6,8,1...|       0|\n",
      "|(116,[2,3,4,6,8,1...|       1|\n",
      "|(116,[3,4,8],[1.0...|       1|\n",
      "|(116,[1,4,5,9],[1...|       0|\n",
      "|(116,[1,4,6,8,22]...|       1|\n",
      "|(116,[1,4,5,8,16]...|       1|\n",
      "|(116,[3,4,8,115],...|       0|\n",
      "|(116,[3,4,8],[1.0...|       1|\n",
      "|(116,[3,4,8,115],...|       0|\n",
      "|(116,[1,3,4,5,8,1...|       0|\n",
      "|(116,[3,4,8],[1.0...|       0|\n",
      "|(116,[3,4,8],[1.0...|       0|\n",
      "|(116,[4,5,8,115],...|       0|\n",
      "|(116,[4,5,8,115],...|       0|\n",
      "|(116,[2,4,5,8,115...|       0|\n",
      "|(116,[1,4,6,8,115...|       0|\n",
      "|(116,[4,7,8],[24....|       0|\n",
      "|(116,[1,4,5,8,115...|       0|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first select only the labels and features we have created to this point\n",
    "df = df.select(['survived', 'class_index', 'age_imputed', 'new_age', 'embarked_index'\n",
    "               ,'room_imputed', 'room_categ', 'room_index', 'boat_index', 'sex_index'\n",
    "               ,'class_vect', 'embarked_vect', 'room_vect', 'boat_vect', 'sex_vect'])\n",
    "\n",
    "# create a single vector only with valid features (no indexes or labels)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['class_vect', 'age_imputed', 'new_age', 'embarked_vect', 'room_imputed', 'room_vect', 'boat_vect', 'sex_vect'],\n",
    "    outputCol='features')\n",
    "\n",
    "final_df = assembler.transform(df)\n",
    "final_df = final_df.select('features', 'survived')\n",
    "\n",
    "print(\"Assembled features:\") \n",
    "print(\"['class_vect', 'age_imputed', 'new_age', 'embarked_vect', 'room_imputed', 'room_vect', 'boat_vect', 'sex_vect']\")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally!!!\n",
    "Now we can create our regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data with an 70/30 ratio\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "train_data, test_data = final_df.randomSplit([0.7, 0.3], seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "lr_titanic = LogisticRegression(featuresCol='features', labelCol='survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "lr_model = lr_titanic.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can evaluate the performance of our model with a roc curve\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr_summary = lr_model.summary\n",
    "roc = lr_summary.roc.toPandas()\n",
    "plt.plot(roc['FPR'], roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve [training] ' + str(lr_summary.areaUnderROC));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get predictions with test_data\n",
    "lr_results = lr_model.transform(test_data)\n",
    "\n",
    "# the schema of results will give us the resulting vectors available for evaluation\n",
    "lr_results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "|       0|       0.0|\n",
      "|       1|       1.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look a the actual predictions\n",
    "lr_results.select('survived', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve [testing]: 0.8635787533125936\n"
     ]
    }
   ],
   "source": [
    "# last, we can evaluate our model with test_data\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='survived')\n",
    "\n",
    "lr_score = lr_eval.evaluate(lr_results)\n",
    "print('Area under the curve [testing]:', lr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's a pretty good model.\n",
    "\n",
    "Now, let's create other different models and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to evaluate performance of Decision Trees, Random Forest and Gradient Boosting\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 3 different models\n",
    "dtc = DecisionTreeClassifier(labelCol='survived', featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='survived', featuresCol='features')\n",
    "gbc = GBTClassifier(labelCol='survived', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbc_model = gbc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions with test_data\n",
    "dtc_preds = dtc_model.transform(test_data)\n",
    "rfc_preds = rfc_model.transform(test_data)\n",
    "gbc_preds = gbc_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- survived: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can see the resulting vectors available for evaluation (actually those are the same from linear_regression)\n",
    "dtc_preds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: 0.8635787533125936\n",
      "Decision Tree Classifier: 0.44316741560087564\n",
      "Random Forest Classifier: 0.8944290816914391\n",
      "Gradient Boosting Classifier: 0.9347707109113952\n"
     ]
    }
   ],
   "source": [
    "# to evaluate our model we use again BinaryClassificationEvaluator\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol='survived')\n",
    "\n",
    "print('Logistic regression:', lr_score)\n",
    "print('Decision Tree Classifier:', binary_eval.evaluate(dtc_preds))\n",
    "print('Random Forest Classifier:', binary_eval.evaluate(rfc_preds))\n",
    "print('Gradient Boosting Classifier:', binary_eval.evaluate(gbc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### surprisingly, random forest performs just above than our logistic regression model\n",
    "\n",
    "### Gradient Boosting Classifier turns out to be the best one among the 4 chosen models\n",
    "\n",
    "### (Logistic Regression, Decision Trees, Random Forest, Gradient Boosting)\n",
    "\n",
    "As a final note, we should remember that we used the default parameters when creating the classification models.\n",
    "There are dozens of parameters we can start tuning in order to get a better score on each one of the models created"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
